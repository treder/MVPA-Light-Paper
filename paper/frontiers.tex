%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This is just an example/guide for you to refer to when submitting manuscripts to Frontiers, it is not mandatory to use Frontiers .cls files nor frontiers.tex  %
% This will only generate the Manuscript, the final article will be typeset by Frontiers after acceptance.
%
%
%
% When submitting your files, remember to upload this *tex file, the pdf generated with it, the *bib file (if bibliography is not within the *tex) and all the figures.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Version 3.4 Generated 2018/06/15 %%%
%%% You will need to have the following packages installed: datetime, fmtcount, etoolbox, fcprefix, which are normally inlcuded in WinEdt. %%%
%%% In http://www.ctan.org/ you can find the packages and how to install them, if necessary. %%%
%%%  NB logo1.jpg is required in the path in order to correctly compile front page header %%%

\documentclass[utf8]{frontiersSCNS} % for Science, Engineering and Humanities and Social Sciences articles
%\documentclass[utf8]{frontiersHLTH} % for Health articles
%\documentclass[utf8]{frontiersFPHY} % for Physics and Applied Mathematics and Statistics articles

%\setcitestyle{square} % for Physics and Applied Mathematics and Statistics articles
%\usepackage{url,hyperref,lineno,microtype,subcaption}
\usepackage{url,hyperref,lineno,microtype}
\usepackage[onehalfspacing]{setspace}

\linenumbers


% Leave a blank line between paragraphs instead of using \\


\def\keyFont{\fontsize{8}{11}\helveticabold }
\def\firstAuthorLast{Treder} %use et al only if is more than 1 author
\def\Authors{Matthias S. Treder}
% Affiliations should be keyed to the author's name with superscript numbers and be listed as follows: Laboratory, Institute, Department, Organization, City, State abbreviation (USA, Canada, Australia), and Country (without detailed address information such as city zip codes or street names).
% If one of the authors has a change of address, list the new address below the correspondence details using a superscript symbol and use the same symbol to indicate the author in the author list.
\def\Address{School of Computer Science \& Informatics, Cardiff University, Cardiff, UK}
% The Corresponding Author should be marked with an asterisk
% Provide the exact contact address (this time including street name and city zip code) and email of the corresponding author
\def\corrAuthor{Corresponding Author}

\def\corrEmail{trederm@cardiff.ac.uk}

%%% --- added by Matthias ---
\usepackage{tabularx}  % controls the table width
\usepackage{multirow}
\usepackage{prettyref}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{bbm}    % for 1 as a vector

\newtheorem{theorem}{Theorem}

% Math symbols
\newcommand{\al}{\boldsymbol{\alpha}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\mf}[2]{m_{#1#2}}
% \newcommand{\mf}[2]{m_#1^{(#2)}}
\newcommand{\mbar}{\overline{\m}}
\newcommand{\mm}[1]{\m_{#1}}
\newcommand{\sig}[2]{\sigma_{#1#2}^2}
% \newcommand{\sig}[2]{(\sigma^2)_#1^{(#2)}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\x}{\mathbf{x}}
% \newcommand{\xf}[1]{\mathbf{x}^{(#1)}} % x features
\newcommand{\xf}[1]{x^{(#1)}} % x features
\newcommand{\y}{\mathbf{y}}
\newcommand{\yhat}{\widehat{\mathbf{y}}}
\newcommand{\cov}{\text{cov}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\E}{\mathbb{E}}   % expectation
\newcommand{\G}{\mathbf{G}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\K}{\mathbf{K}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\N}{\mathcal{N}}   % normal distribution
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\S}{\mathbf{S}}
\newcommand{\Tr}[1]{\text{Tr}(#1)}
\newcommand{\W}{\mathbf{W}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}

\newcommand{\ttt}[1]{\texttt{#1}}

\newrefformat{fig}{Figure \ref{#1}}
\newrefformat{tab}{Table \ref{#1}}
\newrefformat{eq}{Eq. (\ref{#1})}
\newrefformat{app}{Appendix \ref{#1}}
\newrefformat{sec}{Section \ref{#1}}
\newrefformat{lemma}{Lemma \ref{#1}}
\newrefformat{theorem}{Theorem \ref{#1}}
\newrefformat{assumption}{Assumption \ref{#1}}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{red}{\textbf{todo} #1}}

\graphicspath{{../figures/}}


\begin{document}
\onecolumn
\firstpage{1}

%\title[MVPA-Light]{MVPA-Light: out-of-the-box classification of neuroimaging data}
%\title[MVPA-Light]{MVPA-Light: a classification suite for neuroimaging data}
\title[MVPA-Light]{MVPA-Light: a classification and regression toolbox for multi-dimensional data}

\author[\firstAuthorLast ]{\Authors} %This field will be automatically populated
\address{} %This field will be automatically populated
\correspondance{} %This field will be automatically populated

\extraAuth{}% If there are more than 1 corresponding author, comment this line and uncomment the next one.
%\extraAuth{corresponding Author2 \\ Laboratory X2, Institute X2, Department X2, Organization X2, Street X2, City X2 , State XX2 (only USA, Canada and Australia), Zip Code2, X2 Country X2, email2@uni2.edu}


\maketitle


\begin{abstract}

%%% Leave the Abstract empty if your article does not require one, please see the Summary Table for full details.
\section{}
MVPA-Light is a MATLAB toolbox for multivariate pattern analysis (MVPA). It provides native implementations of a range of classifiers and regression models, using modern optimization algorithms. High-level functions allow for the multivariate analysis of multi-dimensional data, including generalization (e.g. time x time) and searchlight analysis. The toolbox performs cross-validation, hyperparameter tuning, and nested preprocessing. It computes various classification and regression metrics and establishes their statistical significance, is modular, easily extendable. Furthermore, it offers interfaces for LIBSVM and LIBLINEAR as well as an integration into the FieldTrip neuroimaging toolbox. After introducing MVPA-Light, example analyses of MEG and fMRI datasets, and benchmarking results on the classifiers and regression models are presented.



\tiny
 \keyFont{ \section{Keywords:} machine learning, classification, decoding, regression, MVPA, regularization, cross-validation, toolbox} %All article types: you may provide up to 8 keywords; at least 5 are mandatory.
\end{abstract}

%%% -------- INTRODUCTION --------
\section{Introduction}

Multivariate pattern analysis (MVPA) refers to a set of multivariate tools for the analysis of brain activity or structure. It draws on supervised learning, a branch of machine learning mainly dealing with classification and regression problems. Multivariate classification has been used in EEG-based brain-computer interfaces since at least the 1980s \citep{Farwell}, but it did not become a mainstream tool in cognitive neuroscience until the late 2000s \citep{Blankertz2011,Lemm2011,Mur2009,Pereira2009}. MVPA was first popularized by the seminal work of Haxby et al. \citep{Haxby2001,Haxby2012,Norman2006}. In an fMRI study, the authors provided evidence that visual categories (such as faces and houses) are associated with distributed representations across multiple brain regions. MVPA is designed to exploit such multivariate patterns by taking into account multiple voxels or channels simultaneously. This constitutes a major difference between MVPA and traditional statistical methods such as t-test and analysis of variance (ANOVA). Traditional statistical tests are often univariate i.e. a test is performed for each dependent variable, for instance voxel or EEG channel, separately. In contrast to MVPA, such tests are blind to the distributed information encoded in the correlations between different spatial locations.

To highlight this difference with an example, consider a hypothetical visual experiment: In each trial, subjects are presented an image of either a face or a house and their brain activity is recorded using fMRI. To make sure that they maintain attention, subjects are instructed to indicate via a button press whether the image represents a face or a house. This experiment will be referred to as 'faces vs houses' throughout this paper.
To investigate the difference between the brain responses to faces vs houses, a t-test can be applied to answer the question "Is the activity \textit{at a specific voxel} different for faces vs houses?". In contrast, MVPA addresses the more general question "Is the \textit{pattern of brain activity} different for faces vs houses?". This example illustrates that univariate statistics and MVPA inhabit opposite ends of a spectrum between \textit{sensitivity} ("Is there an effect?") and \textit{localizability} ("Where is the effect?"). A classical univariate test might be unable to detect a specific effect because it is blind to multivariate dependencies (low sensitivity) but any effect it does detect is perfectly localized to a single voxel. In contrast, MVPA gains statistical power by capitalizing on correlations between different locations (high sensitivity) but it is difficult to attribute an effect to a specific brain location (low localizability). A MVPA technique called \textit{searchlight analysis} (see glossary) attempts to cover the middle ground between these two extremes. As this comparison illustrates, MVPA should be considered as a complement, rather than a competitor, to traditional statistical methods. Finally, there are other ways in which MVPA and traditional statistics differ. For instance, MVPA includes kernel methods that are sensitive to non-linear relationships and it makes extensive use of techniques such as \textit{cross-validation} that control for \textit{overfitting}.

To use MVPA as part of a neuroimaging analysis pipeline, numerous excellent MATLAB toolboxes have been developed over the years, including the Amsterdam Decoding and Modelling Toolbox (ADAM) \citep{Fahrenfort2018FromADAM}, BCILAB \citep{Kothe2013BCILAB:Development}, Berlin BCI toolbox \citep{Blankertz2016TheControl}, CoSMoMVPA \citep{Oosterhof2016CoSMoMVPA:Octave}, Decision Decoding ToolBOX (DDTBOX) \citep{Bode2019ThePotentials}, Donders Machine Learning Toolbox (DMLT) (\texttt{github.com/distrep/DMLT}), Pattern Recognition for Neuroimaging Toolbox (PRoNTo) \citep{Schrouff2013PRoNTo:Toolbox}, and The Decoding Toolbox (TDT) \citep{Hebart2015TheData}. Beyond MATLAB, the currently most popular computer languages for machine learning are Python and R, with outstanding toolboxes such as Scikit Learn \citep{Pedregosa2011Scikit-learn:Python} for Python and Caret \citep{Kuhn2008BuildingPackage} and MLR \citep{Bischl2000Mlr:R} for R. A comprehensive comparison of MVPA-Light with all of these toolboxes is beyond the scope of this paper, but what sets it apart is the adherence to all of the following design principles:

\begin{itemize}
\item \textit{Self-contained}: unlike many toolboxes that provide wrappers for existing classifiers, the backbone of MVPA-Light is  native implementations of various classifiers, regression models, and their corresponding optimization algorithms (Trust-Region Newton, Dual Coordinate Descent). As a result, MVPA-Light works out-of-the-box, without the need for additional toolboxes or  code compilation.
\item \textit{Transparent}: the toolbox has a shallow code base with well-documented functions. In many cases, the function call stack has a depth of two within the toolbox. For instance, a call to \texttt{mv\_classify} using an LDA classifier triggers calls to functions such as  \texttt{mv\_check\_inputs}, \texttt{train\_lda} and \texttt{test\_lda}. Although the train/test functions might call additional optimization functions, most of the work is done at these two shallowest levels. To preserve the shallowness, high-level functions replicate some code that might be shared otherwise. Object orientation and encapsulation is avoided in favour of the more transparent MATLAB structs.
\item \textit{Fast}: all models and high-level functions are written with speed as a prime concern. In some cases, the need for speed conflicts with the out-of-the-box requirement. For instance, Logistic Regression and SVM use iterative optimization algorithms written in MATLAB. However, these algorithms potentially run faster using compiled code. To this end, an interface is provided for LIBSVM \citep{Chang2011LIBSVM:Machines} and LIBLINEAR \citep{Fan2008}, two C implementations of Logistic Regression and SVM for users who do not shy away from compiling the code on their platform.
\item \textit{Modular and pluggable}: it is possible, and intended, to harvest parts of the code such as the classifiers for other purposes. It is also easy to plug the toolbox into a larger neuroimaging analysis framework. An interface for FieldTrip \citep{Oostenveld2011} is described in the Methods section.
\item \textit{High-level interface}: common MVPA tasks such as searchlight analysis and time generalization including cross-validation can be performed with a few lines of MATLAB code. Many of the hyperparameters required by classifiers and regression models are automatically selected by MVPA-Light, taking the burden of hyperparameter selection off the user.
\end{itemize}

It is worth noting that MVPA-Light is a purely statistical toolbox. That is, it assumes that data has been preprocessed with a neuroimaging toolbox and comes in the shape of MATLAB arrays. Many neuroimaging toolboxes (e.g. FieldTrip, SPM, EEGLAB) store the imaging  data in such arrays, so that MVPA-Light can easily be used as a plugin tool. This comes with the perk that adaptation to different imaging modalities is straightforward.

\subsection{MVPA Glossary}

MVPA comes with its own set of commonly used terms, many of which are borrowed from machine learning. Since they are used extensively throughout the paper, a glossary is provided here. Fully understanding these concepts can be challenging so unfamiliar readers are referred to review papers on MVPA  \citep{Mur2009,Misaki2010ComparisonFMRI,Grootswagers2017DecodingData,Pereira2009,Varoquaux2017}. For an in-depth introduction to machine learning refer to standard textbooks \citep{James2013,Hastie2009,Bishop2007}.


\begin{itemize}
\item \textit{Binary classifier}. A classifier trained on data that contains two classes, such as in the 'faces vs houses' experiment. If there is more than two classes, the classifier is called a multi-class classifier.
\item \textit{Classification}. One of the primary applications of MVPA. In classification, a classifier takes a multivariate pattern of brain activity (referred to as \textit{feature vector}) as input and maps it onto a categorical brain state or experimental condition (referred to as \textit{class label}). In the 'faces vs houses' experiment, the classifier is used to investigate whether patterns of brain activity  can discriminate between faces and houses.
\item \textit{Classifier}. An algorithm that performs classification, for instance Linear Discriminant Analysis (LDA) and Support Vector Machine (SVM).
\item \textit{Classifier output}. If a classifier receives a pattern of brain activity (feature vector) as input, its output is a predicted class label e.g. 'face'. Many classifiers are also able to produce class probabilities (representing the probability that a brain pattern belongs to a specific class) or decision values.
\item \textit{Class label}. Categorical variable that represents a label for each sample/trial. In the 'faces vs houses' experiment, the class labels are 'face' and 'house'. Class labels are often encoded by numbers, e.g. 'face' = 1 and 'house' = 2, and arranged as a vector. For instance, the class label vector  [1, 2, 1] indicates that a subject viewed a face in trial 1, a house in trial 2, and another face in trial 3.
\item \textit{Cross-validation}. To obtain a realistic estimate of classification or regression performance and control for overfitting, a model should be tested on an independent dataset that has not been used for training. In most neuroimaging experiments, there is only one dataset with a restricted number of trials. K-fold cross-validation makes efficient use of such data by splitting it into k different folds. In every iteration, one of the k folds is held out and used as test set, whereas all other folds are used for training. This is repeated until every fold served as test set once. Since cross-validation itself is stochastic due to the random assignment of samples to folds, it can be useful to repeat the cross-validation several times and average the results. See \cite{Lemm2011,Varoquaux2017} for a discussion of cross-validation and potential pitfalls.
\item \textit{Data}. From the perspective of a classifier or regression model, a dataset is a collection of samples (e.g. trials in an experiment). Each sample consists of a brain pattern and a corresponding class label or response. In formal notation, each sample consists of a pair $(\x, y)$ where $\x$ is a feature vector and $y$ is the corresponding class label or response.
\item \textit{Decision boundary}. Classifiers partition feature space into separate regions. Each region is assigned to a specific class. Classifiers make predictions for a test sample by looking up into which region it falls. The boundary between regions is known as decision boundary. For linear classifiers, the decision boundary is also known as a hyperplane.
\item \textit{Decision value}. Classifiers such as LDA and SVM produce decision values which can be thresholded to produce class labels. For linear classifiers and kernel classifiers, a decision value represents the distance to the decision boundary. The further away a test sample is from the decision boundary, the more confident the classifier is about it belonging to a particular class. Decision values are unitless.
\item \textit{Decoder}. An alternative term for a \textit{classifier} or \textit{regression model} that is popular in the neuroimaging literature. The term nicely captures the fact that it tries to invert the encoding process. In encoding e.g. a sensory experience such as viewing a face is translated into a pattern of brain activity. In decoding, one starts from a pattern of brain activity and tries to infer whether it was caused by a face or a house stimulus.
\item \textit{Feature}. A feature is a variable that is part of the input to a model. If the dataset is tabular with rows representing samples, it typically corresponds to one of the columns. In the 'faces vs houses' experiment, each voxel represents a feature.
\item \textit{Feature space}. Usually a real vector space that contains the feature vectors. The dimensionality of the feature space is equal to the number of features.
\item \textit{Feature vector}. For each sample, features are stored in a vector. E.g. consider a EEG measurement with three electrodes Fz, Cz, and Oz and corresponding voltages 40 $\mu$V, 65 $\mu$V, and 97 $\mu$V. The voltage at each EEG sensor represents a feature, so the corresponding feature vector is the vector [40, 65, 97]$\in\R^3$.
\item \textit{Fitting (a model)}. Same as \textit{training}.
\item \textit{Hyperparameter}. A parameter of a model that needs to be specified by the user, such as the type and amount of regularization applied, the type of kernel, and the kernel width $\gamma$ for Gaussian kernels. From the user's perspective, hyperparameters can be nuisance parameters: it is sometimes not clear a priori how to set them, but their exact value can have a substantial effect on the performance of the model.
\item \textit{Hyperparameter tuning}. If it is unclear how a hyperparameter should be set, multiple candidate values can be tested. Typically, this is done via nested cross-validation: the training set is again split into separate folds. A model is trained for each of the candidate values and its performance is evaluated on the held-out fold, called validation set. Only the model with the best performance is then taken forward to the test set.
\item \textit{Hyperplane}. For linear classifiers, the decision boundary is a hyperplane. In the special case of a two-dimensional feature space, a hyperplane corresponds to a straight line. In three dimensions, it corresponds to a plane. 
\item \textit{Loss function}. A function that is used for training. The model parameters are optimized such that the loss function attains a minimum value. For instance, in Linear Regression the sum of squares of the residuals serves as a loss function.
\item \textit{Metric}. A quantitative measure of the performance of a model on a test set. E.g. precision/recall for classification or mean squared error for regression.
\item \textit{Model}. In the context of this paper, a model is a classifier or regression model.
\item \textit{Multi-class classifier}. A classifier trained on data that contains three or more classes. For instance, assume that in the 'faces vs houses' experiment additional images have been presented depicting 'animals' and 'tools'. This would define four classes in total, hence  classification would require a multi-class classifier.
\item \textit{Overfitting}. Occurs when a model over-adapts to the training data. As a consequence, it will perform well on the training set but badly on the test set. Generally speaking, overfitting is more likely to occur if the number of features is larger than the number of samples, and more likely for complex non-linear models than for linear models. Regularization can serve as an antidote to overfitting.
\item \textit{Parameters}. Models are governed by parameters e.g. beta coefficients in Linear Regression or the weight vector $\w$ and bias $b$ in a linear classifier.
\item \textit{Regression}. One of the primary applications of MVPA (together with classification). Regression is very similar to classification, but it aims to predict a continuous variable rather than a  class label. For instance, in the 'faces vs houses' experiment, assume that the reaction time of the button press has been recorded, too. To investigate the question "Does the pattern of brain activity in each trial predict reaction time?", regression can be performed using reaction time as responses.
\item \textit{Regression model}. An algorithm that performs regression, for instance Ridge Regression and Support Vector Regression (SVR).
\item \textit{Regularization}. A set of techniques that aim to reduce overfitting. Regularization is often directly incorporated into training by adding a penalty term to the loss function. For instance, L1 and L2 penalty terms are popular regularization techniques. They reduce overfitting by preventing coefficients from taking on too large values.
\item \textit{Response}. In regression, responses act as the target values that a model tries to predict. They play the same role that class labels play in classification. Unlike class labels, responses are continuous e.g. reaction time.
\item\textit{Searchlight analysis}. In neuroimaging analysis, a question such as "Does brain activity differentiate between faces and houses?" is usually less interesting than the question "Which brain regions differentiate between faces and houses?". In other words, the goal of MVPA is to establish the presence of an effect \textit{and} localize it in space or time. Searchlight analysis intends to marry statistical sensitivity with localizability. It is a well-established technique in the fMRI literature, where a searchlight is defined e.g. as a sphere of 1 cm radius, centered on a voxel in the brain \citep{Kriegeskorte2006Information-basedMapping}. All voxels within the radius serve as features for a classification or regression analysis. The result of the analysis is assigned to the central voxel. If the analysis is repeated for all voxel positions, the resultant 3D map of classification accuracies can be overlayed on a brain image. Brain regions that have discriminative information then light up as peaks in the map. Searchlight analysis is not limited to spatial coordinates. The same idea can be applied to other dimensions such as time points and frequencies.
\item \textit{Testing}. The process of applying a trained model to the test set. The performance of the model can then be quantified using a metric.
\item \textit{Test set}. Part of the data designated for testing. Like with training sets, test sets are automatically defined in cross-validation, or they can arise naturally in multi-site studies or in experiments with different phases. 
\item \textit{Training}. The process of optimizing the parameters of a model using a training set.
\item \textit{Training set}. Part of the data designated for training. In cross-validation, a dataset is automatically split into training and test sets. In other cases, a training set may arise naturally. For instance, in experiments with different phases (e.g. memory encoding and memory retrieval) one phase may serve as training set and the other phase as test set. Another example is multi-site studies, where a model can be trained on data from one site and tested on data from another site.
\item \textit{Underfitting}. Occurs when a classifier or regression model is too simple to explain the data. E.g. imagine a dataset wherein the optimal decision boundary is a circle, with samples of class 1 being inside the circle and samples of class 2 outside. A linear classifier is not able to represent a circular decision boundary, hence it will be unable to adequately solve the task. Underfitting can be checked by fitting a complex model (e.g. kernel SVM) to data. If the complex model performs much better than a more simple linear model (e.g. LDA) then it is likely that the simple model underfits the data. In most neuroimaging datasets, overfitting is more of a concern than underfitting.
\end{itemize}


The rest of the paper is structured as follows. The high-level functions of the toolbox are described, followed by an introduction of the classifiers and regression models. Then, example analyses are presented using a publicly available \cite{Wakeman2014OpenfMRI,Wakeman2015ADataset} MEEG dataset and the \cite{Haxby2001} fMRI dataset. Finally, a benchmarking analysis is conducted wherein the computational efficiency of the classifiers and regression models in MVPA-Light is compared to models in other toolboxes in MATLAB, Python, and R.


%%% ------------------------
%%% ------------------------
%%% -------- METHOD --------
%%% ------------------------
%%% ------------------------
\section{Materials and methods}

\subsection{Requirements}

A standard desktop computer is sufficient to run MVPA-Light. The RAM requirement is dictated by the memory footprint of the dataset. Since some functions operate on a copy of the data, it is recommended that the available RAM exceeds the size of the dataset by at least a factor of two (e.g. 4+ GB RAM for a 2 GB dataset). MVPA-Light is supported by MATLAB 2012a and more recent versions. The Statistics toolbox is required at some points in the toolbox (e.g. for calculating t-values). The cluster permutation test in \ttt{mv\_statistics} uses the Image Processing toolbox to extract the clusters.

\subsection{Getting Started}

MVPA-Light is shipped with a set of example scripts (in the \ttt{/examples} subfolder) and an example EEG dataset. These scripts cover both the high-level functions in MVPA-Light and calling the train/test functions manually. The best starting point is to work through the example scripts and then adapt them to one's purpose. An up-to-date introduction to the toolbox with relevant hyperlinks is provided on the GitHub page (\ttt{github.com/treder/mvpa-light}).

The EEG data has been taken from the BNCI-Horizon-2020 repository (http://bnci-horizon-2020.eu/database). It consists of three \ttt{mat} files corresponding to three subjects (subject codes \ttt{VPaak}, \ttt{VPaan}, and \ttt{VPgcc}) from the auditory oddball paradigm introduced in \cite{Treder2014}. Out of the experimental conditions, the "SynthPop" condition has been selected. Attended and unattended deviants are coded as class 1 and 2. The 64 EEG channels in the original dataset have been reduced to 32 channels.

To give a concrete code example, consider the 'faces vs houses' experiment. For each trial, the BOLD response has been recorded for all voxels. This yields a [samples x voxels] data matrix for one subject, where the samples correspond to trials and the voxels serve as features. The matrix is denoted as \ttt{X}. Each trial corresponds to either a 'face' or a 'house' stimulus. This is encoded in a vector of class labels, denoted as \ttt{clabel}, that contains 1's and 2's ('face' = 1, 'house' = 2). Then the following piece of code performs 10-fold cross-validation with 2 repetitions. LDA is used as classifier and area under the ROC curve (AUC) is calculated as a classification metric.

\begin{verbatim}
cfg = [];
cfg.model      = 'lda';
cfg.metric     = 'auc';
cfg.cv         = 'kfold';
cfg.k          = 10;
cfg.repeat     = 2;

auc = mv_classify(cfg, X, clabel);
\end{verbatim}

The output value \ttt{auc} contains the classifier performance measure, in this case a single AUC value averaged across test folds and repetitions. \ttt{mv\_classify} is part of the high-level interface that will be discussed next.

%%% -----------------------------
%%% --- HIGH-LEVEL FUNCTIONS ----
%%% -----------------------------
\subsection{High-level Interface}\label{sec:interface}

The structure of MVPA-Light is depicted in Figure \ref{fig:structure}. The toolbox can be interacted with through high-level functions that cover common classification tasks. \ttt{mv\_classify} is a  general-purpose function that works on data of arbitrary dimension (e.g. time-frequency data). It performs any combination of cross-validation, searchlight analysis, generalization, and other tasks. Two more specialized functions are provided for convenience: \ttt{mv\_classify\_across\_time}  and \ttt{mv\_classify\_timextime}, assume that the data has a time dimension i.e. it is a 3-D [samples $\times$ features $\times$ time points] array. \ttt{mv\_classify\_across\_time} performs classification for every time point, resulting in a vector of cross-validated metrics, the length of the vector being the number of time points. \ttt{mv\_classify\_timextime} expects the same 3-D input. It implements time generalization \citep{King2014} i.e. classification for every combination of training and test time points, resulting in a 2-D matrix of cross-validated metrics. For regression tasks, the equivalent to \ttt{mv\_classify} is the function \ttt{mv\_regress}. It also works with data of arbitrary dimension and supports both searchlight and generalization.

All high-level functions take three input arguments. First, \ttt{cfg}, a configuration structure wherein parameters for the analysis can be set. Second, \ttt{X}, the data acting as input to the model. Third, \ttt{clabel} or \ttt{y}, a vector of class labels or responses. Some of the parameters in the \ttt{cfg} struct are common  to all high-level functions:

\begin{itemize}
    \item \ttt{cfg.model}: name of the classifier or regression model, e.g. \ttt{'lda'}.
    \item \ttt{cfg.hyperparameter}: a struct that specifies the hyperparameters for the model. For instance, \ttt{cfg.hyperparameter.lambda = 0.1} sets the magnitude of shrinkage regularization in LDA. LDA's hyperparameters are introduced in \prettyref{sec:lda}.
    \item \ttt{cfg.metric}: specifies the metric to be calculated from the model predictions e.g. classification accuracy or mean-squared error for regression. Metrics are introduced in \prettyref{sec:metrics}.
    \item \ttt{cfg.preprocess}: a struct that specifies a nested preprocessing pipeline. The pipeline consists of preprocessing operations that are applied on train and test data separately. Preprocessing is discussed in \prettyref{sec:preprocessing}.
\end{itemize}



\subsubsection{Cross-Validation}

Cross-validation is implemented in all high-level functions. It is controlled by the following parameters that are part of the \ttt{cfg} struct defined in the previous section:

\begin{itemize}
    \item \ttt{cfg.cv}: cross-validation type, either \ttt{'kfold'}, \ttt{'leaveout'}, \ttt{'predefined'}, \ttt{'holdout'} or \ttt{'none'}.
    \item \ttt{cfg.k}: number of folds in k-fold cross-validation.
    \item \ttt{cfg.repeat}: number of times the cross-validation is repeated with new randomly assigned folds.
    \item \ttt{cfg.p}: if \ttt{cfg.cv = 'holdout'}, \ttt{p} is the fraction of test samples.
    \item \ttt{cfg.fold}: if \ttt{cfg.cv = 'predefined'}, fold is a vector of integers that  specifies which fold a sample belongs to.
    \item \ttt{cfg.stratify}: if 1, for classification, the class proportions are approximately preserved in each test fold.
\end{itemize}

See the function \ttt{mv\_get\_crossvalidation\_folds} for more details.

\subsubsection{Hyperparameter Tuning}

MVPA-Light tries to automate hyperparameter selection as much as possible. This is done using either reasonable default values, hyperparameter estimators (\cite{Ledoit2004HoneyMatrix} for LDA) or  hyperparameter-free regularizers (log-F(1,1) for Logistic Regression). If this is not possible, automated grid search using nested cross-validation can be used for testing out different hyperparameter combinations essentially by brute force. For better performance, bespoke hyperparameter tuning functions are implemented for some classifiers. Otherwise, the generic tuning function \ttt{mv\_tune\_hyperparameter} is used.

%%% ------- PREPROCESSING -------
\subsubsection{Preprocessing}\label{sec:preprocessing}

Preprocessing refers to operations applied to the data prior to training the classifier. To not bias the result, some preprocessing operations (such as Common Spatial Patterns) should be performed in a 'nested' fashion. That is, they are performed on the training data first and subsequently applied to the test data using parameters estimated from the training data \citep{Lemm2011,Varoquaux2017}.
Currently implemented functions include PCA, sample averaging \citep{Cichy2017MultivariateSpace}, kernel averaging \citep{Treder2018}, and under-/oversampling for unbalanced data. Preprocessing pipelines are defined by adding the \ttt{cfg.preprocess} parameter. For instance,

\begin{verbatim}
cfg.preprocess = {'undersample', 'zscore', 'average_kernel'}
\end{verbatim}

adds a preprocessing pipeline that perfoms undersampling of the data followed by z-scoring and kernel averaging.

%%% ------- SEARCHLIGHT -------
\subsubsection{Searchlight Analysis}\label{sec:searchlight}


In MVPA-Light, \ttt{mv\_classify\_across\_time} performs searchlight analysis across the time axis. More bespoke searchlight analyses can be conducted using \ttt{mv\_classify} and \ttt{mv\_regress} by setting the parameter \ttt{cfg.neighbours}.

%%% -----------------------------
%%% -------- CLASSIFIERS --------
%%% -----------------------------
\subsection{Classifiers}\label{sec:classifiers}

The main workhorses of MVPA are classifiers and regression models. Figure \ref{fig:classifiers} provides a pictorial description of the classifiers. They are implemented using pairs of train/test functions. In the high-level interface, a classifier and its hyperparameters can be specified using \ttt{cfg.model} and \ttt{cfg.hyperparameter}. For instance,

\begin{verbatim}
cfg.model = 'lda';
cfg.hyperparameter.lambda = 0.1;
\end{verbatim}

specifies an LDA classifier and sets the hyperparameter \ttt{lambda = 0.1}. The \ttt{cfg} struct can then be used in a high-level function call, e.g. \ttt{acc = mv\_classify\_across\_time(cfg, X, clabel)}.
Alternatively, as a low-level interface, the train/test functions can be called directly. For instance, an LDA classifier can be trained directly using

\begin{verbatim}
model = train_lda(param, X, clabel)
\end{verbatim}

where \ttt{X} is the training data and \ttt{clabel} are the corresponding class labels. \ttt{param} is a MATLAB struct that contains hyperparameters (same as \ttt{cfg.hyperparameter}). It can be initialized by calling \ttt{param = mv\_get\_hyperparameter('lda')}. An explanation of the hyperparameters for LDA is given when typing \ttt{help('train\_lda')} in MATLAB. 
The output \ttt{model} is a struct that contains the classifier's parameters after training. The classifier can be applied to test data, denoted as \ttt{Xtest}, by calling

\begin{verbatim}
[clabel, dval, prob] = test_lda(model, Xtest)
\end{verbatim}

The first output argument \ttt{clabel} is the predicted class labels. They can be compared against the true class labels to calculate a classification performance metric. \ttt{test\_lda} provides two additional outputs, but not all classifiers have this capability. \ttt{dval} is the decision value, a dimensionless quantity that measures the distance to the hyperplane. \ttt{prob} contains the probability for a given sample to belong to class 1.

To introduce some mathematical notation needed in the following, data is denoted as a matrix $\X\in\R^{n \times p}$ of $n$ samples and $p$ predictors/features. The i-th row of $\X$ is denoted as the column vector $\x_i\in\R^p$. Class labels are stored in a vector $\mathbf{y}\in\R^n$ with $y_i$ referring to the i-th class label. When the index is not relevant, the feature vector and class label are simply referred to as $\x$ and $y$. Before describing the classifiers, two conceptual perspectives are introduced that highlight some of their similarities.

\subsubsection{Perspective 1: Linear Classifiers}

For two classes, linear classifiers such as LDA, Logistic Regression, and linear SVM act on the data in a unified way. The decision value for a test sample $\x$ is given by

\begin{equation}
\label{eq:linear_dval}
\text{dval} = \w^\top\x + b
\end{equation}

where $\w$ is the weight vector or normal to the hyperplane specifying the linear combination of features, and $b$ is the threshold/bias term.  A sample is assigned to the first class if \ttt{dval}$>0$ and to the second class if \ttt{dval}$<0$. If we encode class 1 as +1 and class 2 as -1, this can be expressed concisely as

\begin{equation*}
\text{predicted class} =\text{sign} \left(\w^\top\x + b\,\right)
\end{equation*}

where $\text{sign}:\R\rightarrow\{-1,+1\}$ is the sign function. Linear classifiers differ only in the way that $\w$ and $b$ are derived.

\subsubsection{Perspective 2: Probabilistic Classifiers}

Another useful perspective is given by the Bayesian framework \citep{Bishop2007}. Probabilistic classifiers such as LDA, Naive Bayes, and Logistic Regression are able to directly model class probabilities for individual samples. Let us denote the (posterior) probability for class $i$ given  test sample $\x$ as $P(y = i\,|\,\x)$. A possible approach for calculating this quantity is Bayes' theorem:

\begin{equation}
\label{eq:bayes}
P(y = i\,|\,\x) = \frac{P(\x\,|\,y = i)\  P(y = i)}{P(\x)}
\end{equation}

Here, $P(\x\,|\,y = i)$ is the likelihood function which quantifies the relative probability of observing $\x$ given the class label, and $P(y = i)$ is the prior probability for a sample to belong to class $i$. The denominator, called evidence, can be calculated by marginalizing across the classes: $P(\x) = \sum_i P(\x\,|\,y = i)\  P(y = i)$.

%%% -------- LDA --------
\subsubsection{Linear Discriminant Analysis (LDA)}\label{sec:lda}

If the classes follow a multivariate Gaussian distribution with a common covariance matrix for all classes, LDA yields the theoretically optimal classifier \citep{Duda1998}. In the context of EEG/MEG analysis, LDA is discussed in detail in \cite{Blankertz2011}. The likelihood function takes the form

\begin{equation}
\label{eq:lda_likelihood}
P(\x\,|\,y = i) \sim \mathcal{N}(\m_i, \mathbf{\Sigma})
\end{equation}

i.e. it is multivariate Gaussian distributed with a class-specific mean $\m_i$ and common covariance matrix $\mathbf{\Sigma}$. Both need to be estimated from the training data. \prettyref{eq:bayes} can then be evaluated to calculate class probabilities. A prediction can be done by selecting the most likely class out of all candidate classes,

\begin{equation*}
\text{predicted class} = \underset{i}{\text{arg max}}\ P(y = i\,|\,\x)
\end{equation*}

which is known as the maximum a posteriori (MAP) rule. LDA is closely related to other statistical models. For two classes, LDA is equivalent to Linear Regression using the class labels as targets. It is also equivalent to Linearly Constrained Minimum Variance (LCMV) beamforming when applied to ERP data \citep{Treder2016}. The latter equivalence relationship also applies to other methods based on generalized eigenvalue decomposition of covariance matrices \citep{DeCheveigne2014JointAnalysis}.

In MVPA-Light, multi-class LDA is implemented as the classifier  \texttt{'multiclass\_lda'}. For two classes, a more efficient implementation denoted as  \ttt{'lda'} is available. In practice, the covariance matrix is often ill-conditioned and needs to be regularized \citep{Blankertz2011}. The hyperparameter \ttt{lambda} controls the amount of regularization. In shrinkage regularization,  \ttt{lambda} $\in [0,1]$ blends between the empirical covariance matrix (\ttt{lambda}=0) and a scaled identity matrix (\ttt{lambda}=1). By default, \ttt{lambda} is estimated automatically using the Ledoit-Wolf formula \citep{Ledoit2004HoneyMatrix}. \prettyref{sec:lda_implementation} discusses the implementation of LDA in detail.


%%% -------- NAIVE BAYES --------
\subsubsection{Naive Bayes}

In Naive Bayes, the features are assumed to be conditionally independent of each other given the class label \citep{Bishop2007}. While this is indeed naive and often wrong, Naive Bayes has nevertheless been remarkably successful in classification problems. The independence assumption leads to a straightforward formula for the likelihood function since only univariate densities need to be estimated. Let $\xf{j}$ be the $j$-th feature and $\x = [\xf{1}, \xf{2}, ..., \xf{p}]^\top$ be a feature vector then the likelihood function is given by

\begin{equation*}
P(\x\,|\,y = i) = \prod_{j=1}^p P(\xf{j}\,|\,y = i)
\end{equation*}

Like in LDA, the predicted class can be obtained using the MAP rule. In MVPA-Light, Naive Bayes is implemented as \texttt{'naive\_bayes'}. Additionally, MVPA-Light assumes that these densities are univariate Gaussian i.e. $P(\xf{j}\,|\,y = i) \sim \mathcal{N}(m_{ij}, \sigma^2_{ij})$. For Gaussian densities, the independence assumption is equivalent to assuming that the covariance matrix is diagonal. As indicated in Figure \ref{fig:classifiers}, there is a close relationship between LDA and Gaussian Naive Bayes: LDA allows for a dense covariance matrix, but it requires that it is the same for all classes. In contrast, Naive Bayes allows each class to have a different covariance matrix, but it requires each matrix to be diagonal. Additional details on the implementation are given in section 4.2 (Appendix).

%%% -------- LOGISTIC REGRESSION --------
\subsubsection{Logistic Regression}

In Logistic Regression for two classes, the posterior probability is modeled directly by fitting a logistic function to the data \citep{Hastie2009}. If the two classes are coded as +1 and -1, it is given by

\begin{equation}
\label{eq:logreg_probability}
P(y = \pm 1\,|\,\x) = \frac{1}{1 + \text{exp}(-y\,(\w^\top\x + b))}
\end{equation}

The weights $\w$ are found by minimizing the logistic loss function

\begin{equation}
\label{eq:logreg_loss_function}
\L_\text{LR}(\w) = \sum_{i=1}^n \log[1 + \text{exp}(-y_i(\w^\top\x_i + b))]
\end{equation}

In MVPA-Light, Logistic Regression  is implemented as \texttt{'logreg'}. By default, log-F(1,1) regularization (\ttt{reg = 'logf'}) is used by imposing Jeffrey's prior on the weights \citep{Firth1993BiasEstimates,Rahman2017PerformanceData.,King2001}. Alternatively, L2-regularization can be used to impose a Gaussian prior on the weights (\ttt{reg = 'l2'}). In this case, an additional hyperparameter \ttt{lambda} $\in [0,\infty)$ that controls the amount of regularization needs to be specified by the user. It can be set to a fixed value. Alternatively, a range of candidates can be specified (e.g. \ttt{lambda = [0.001, 0.01, 0.1, 1]}). A nested cross-validation is then performed to select the optimal value. Additional details on the implementation are given in section 4.3 (Appendix). An alternative implementation using LIBLINEAR is also available, see \prettyref{sec:libsvm}.

%%% -------- SVM --------
\subsubsection{Linear Support Vector Machine (SVM)}

A SVM has no underlying probabilistic model. Instead, it is based on the idea of maximizing the margin \citep{Scholkopf2001LearningBeyond,Hearst1998SupportMachines}. For linearly separable data, the margin is the distance from the hyperplane to the closest data point (dotted line in Figure \ref{fig:classifiers}). This distance is given by $1/||\w||$. Minimizing $||\w||$ is then equal to maximizing the margin. At the same time, one needs to make sure that the training samples are correctly classified at a distance from the hyperplane. This is achieved by requiring
$\w^\top\x_i + b\ge 1$ for class 1 and $\w^\top\x_i + b\le -1$ for class 2. Encoding the classes as +1 and -1, both terms can be combined into $y_i\, (\w^\top\x_i + b) \ge 1$. This constraint cannot be satisfied for every training sample $i\in\{1,...,n\}$ if the data cannot be perfectly separated. Therefore, positive slack variables $\xi_i$ are introduced that allow for misclassifications. Now the goal becomes to maximize the margin while simultaneously minimizing the amount of constraint violations given by $\sum_i \xi_i$. Put together, this leads to the following optimization problem:

\begin{align}
\begin{split}
\label{eq:svm_primal}
\underset{\w}{\text{arg min}} &\ \frac{1}{2}||\w||^2 + c\sum_i \xi_i\\
\text{subject to } &\ \forall i: y_i (\w^\top\x_i + b) \ge 1 - \xi_i\\
&\ \forall i: \xi_i \ge 0\\
\end{split}
\end{align}

The resultant classifier, called two-class L1-Support Vector Machine (SVM) is implemented as \ttt{'svm'}. The hyperparameter \ttt{c} controls the amount of regularization and needs to be set by the user. Despite the lack of a probabilistic model, a Platt approximation using an external function (\ttt{http://www.work.caltech.edu/~htlin/program/libsvm/}) is used to estimate class probabilities if required. Additional details on the implementation are given in section 4.4 (Appendix). Alternative implementations using LIBSVM and LIBLINEAR are also available, see section \ref{sec:libsvm}.

%%% -------- KERNEL METHODS --------
\subsubsection{Kernel Classifiers}\label{sec:kernel-methods}

In kernel methods such as SVM and kernel FDA, a sample is implicitly mapped from the input space $\mathcal{X}$ into a high-dimensional feature space $\mathcal{F}$ using a map $\phi:\mathcal{X}\rightarrow\mathcal{F}$. As illustrated in Figure \ref{fig:classifiers}, such a map can translate a non-linear classification problem into a linear problem in feature space \citep{Scholkopf2001LearningBeyond}. For two classes, decision values are given by

\begin{equation}
\label{eq:kernel_dval}
\text{dval} = \w_\phi^\top\phi(\x) + b
\end{equation}

where $\w_\phi$ is the weight vector in feature space. If we compare this formula to \prettyref{eq:linear_dval}, it becomes evident that kernel classifiers are linear classifiers acting on non-linear transformations of the features. Often, it is  infeasible to explicitly apply the map due to the high dimensionality of $\mathcal{F}$.  However, for methods such as SVM and LDA, an efficient workaround is available. The optimization problem can be rewritten into a form wherein only the inner products between pairs of samples are needed, i.e. $\langle\phi(\x), \phi(\x')\rangle$ for samples $\x$ and $\x'$. Now, if $\phi$ maps to a Reproducing Kernel Hilbert Space (RKHS), these inner products can be efficiently calculated via a kernel function $k$ that operates in input space, resulting in the identity $k(\x,\x') = \langle\phi(\x), \phi(\x')\rangle$. This is known as the \textit{kernel trick}.

To give a simple example, consider two samples with  two-dimensional features, $\x = [x_1, x_2]$ and $\x' = [x_1', x_2']$. The homogeneous polynomial kernel of degree 2 has the kernel function $k(\x,\x') = (\sum_{i=1}^2 x_i x_i')^2$ and the corresponding feature map $\phi:\R^2\rightarrow\R^3$ with $\phi(\x)= [x_1^2, \sqrt{2}x_1 x_2, x_2^2]$. It is now easily verified that $k(\x,\x') = \langle\phi(\x), \phi(\x')\rangle$. For LDA, a kernelized version called Kernel Fisher Discriminant Analysis (KFDA) has been developed by \cite{Mika1999FisherKernels}. It is available as \ttt{'kernel\_fda'}. By default, the model is regularized using shrinkage regularization controlled by the hyperparameter \ttt{lambda}. Often, a small value (e.g. \ttt{lambda = 0.01}) is adequate. Additional details on the implementation are given in section 4.5 (Appendix).
For kernel SVM, either \ttt{'svm'} or the LIBSVM interface can be used. For both SVM and KFDA, the kernel can be chosen by setting the \ttt{kernel} parameter. Further information on the kernels is provided in the train functions.

%%% -------- ENSEMBLES --------
\subsubsection{Ensemble Methods}

An \ttt{'ensemble'} is a meta-classifier that trains dozens or even hundreds of classifiers. In ensembles, these individual classifiers are referred to as learners. The type of learner can be set using the \ttt{learner} hyperparameter. For instance, setting \ttt{learner = 'svm'} creates an ensemble of SVM classifiers. To encourage the learners to focus on different aspects of the data, every learner is presented just a subset of the training data. \ttt{nsamples} controls the number of training samples that is randomly selected for a given learner, whereas \ttt{nfeatures} controls the number of features.
The final classifier output is determined via a voting strategy. If \ttt{strategy = 'vote'}, then the class label produced by each individual learner serves as a vote. The class that receives the maximum number of votes is then selected. If \ttt{strategy = 'dval'}  then the raw decision values are averaged and the final decision is taken based on whether the average is positive or negative. The latter only works with classifiers that produce decision values. 


%%% ---------------------------------
%%% ----   CLASSIFIER OUTPUT     ----
%%% ---------------------------------
\subsubsection{Classifier Output Type}\label{sec:output}

For every test sample, a classifier produces raw output. This output takes either a discrete form as a \textit{class label} or a continuous one. If it is continuous, it comes either as a \textit{decision value} or as a \textit{probability}. A decision value is an unbounded number that can be positive or negative. Its absolute value corresponds to the distance to the hyperplane. For two classes, the probability is a number between 0 and 1 representing the probability that a sample belongs to class 1.
In the high-level interface, the classifier output can be specified explicitly by setting \ttt{cfg.output\_type} to \ttt{'clabel'}, \ttt{'dval'}, or \ttt{'prob'}. In most cases, however, it suffices to let MVPA-Light infer the output type.

%%% -----------------------------
%%% -------- REGRESSION  --------
%%% -----------------------------
\subsection{Regression Models}\label{sec:regression}

Like classifiers, regression models are implemented using pairs of train/test functions. In the high-level function \ttt{mv\_regress}, a regression model is specified using the \ttt{cfg.model} parameter. Low-level access is possible by directly calling the train/test functions. For instance, \ttt{model = train\_ridge(param, X, y)} trains a ridge regression model. \ttt{X} is the training data and \ttt{y} are the corresponding responses. \ttt{param} is a MATLAB struct that contains hyperparameters. The output \ttt{model} is a struct that contains the model parameters after training. The model can be applied to test data by calling \ttt{yhat = test\_ridge(model, Xtest)} where \ttt{Xtest} is test data. The output of the test function is the model predictions. In the following section, the individual regression models are introduced. It is assumed that the training data is contained in matrix $\X\in\R^{n \times p}$ of $n$ samples and $p$ predictors. The i-th row of this matrix is denoted as the column vector $\x_i\in\R^p$. Responses are stored in a vector $\y\in\R^n$ with $\y_i$ referring to the i-th response.

%%% -------- LINEAR REGRESSION --------
\subsubsection{Perspective: Linear Regression}

Linear models such as Linear Regression, Ridge Regression, and linear Support Vector Regression, act on the data in a unified way by means of a vector of coefficients $\w$ (often represented by $\beta$'s in the literature). Linear regression models differ only in the way that $\w$ is derived. To simplify the notation, it is assumed that the data matrix $\X$ contains a column of ones and hence the intercept term is contained in $\w$. For a test sample $\x$, the predicted response is given by $\widehat{y} = \w^\top\x$. The vector of predicted responses on the training data $\yhat\in\R^n$ can be written in matrix notation as 

\begin{equation}
\label{eq:linreg_prediction}
\yhat = \X\w
\end{equation}

During training, the goal is to find a $\w$ such that $y_i \approx\widehat{y}_i$ for each training sample. A natural measure of closeness between the true response and the prediction is the squared distance $(y_i - \widehat{y}_i)^2$, which directly leads to the sum of squares measure $\sum_{i=1}^n (y_i - \widehat{y}_i)^2$. In matrix notation, the sum of squares is denoted as

\begin{equation}
\label{eq:linreg_loss_function}
\L_\text{OLS}(\w) = ||\y - \X\w||^2
\end{equation}

The solution that minimizes this quantity, known as ordinary least squares (OLS) solution to linear regression, is given by $\w = (\X^\top\X)^{-1}\ \X^\top\y$. It is worth noting that if one divides the sum of squares by the number of samples $n$, one obtains the regression metric \textit{mean squared error} (MSE).

%%% -------- RIDGE --------
\subsubsection{Ridge Regression}

Ridge regression is a regularized version of OLS regression. It is useful for data that suffers from multicollinearity. The model is regularized by adding a L2 penalty that shrinks the weights towards zero. For a given regularization parameter \ttt{lambda}$\in[0,\infty)$, denoted by the Greek symbol $\lambda$, the loss function is given by

\begin{equation}
\label{eq:ridge_loss_function}
\L_\text{ridge}(\w) = ||\y - \X\,\w||^2 + \lambda ||\w||^2
\end{equation}

This convex optimization problem can be solved directly by calculating the gradient and setting it to zero. Alternatively, it can be rewritten into its dual Lagrangian form first  
\citep{Bishop2007}. The resultant primal and dual ridge solutions that minimize the loss function are given by

\begin{align}
\begin{split}
\label{eq:ridge}
\w =\ & (\X^\top\X + \lambda\I_p)^{-1}\ \X^\top\y \quad\text{(primal solution)}\\
 =\ & \X^\top (\X\X^\top + \lambda\I_n)^{-1}\ \y \quad\text{(dual solution)}
\end{split}
\end{align}

where $\I_p\in\R^{p\times p}$ and $\I_n\in\R^{n\times n}$ are identity matrices. The equivalence between the primal and dual solution can be verified by left-multiplying both solutions with $(\X^\top\X + \lambda\I_p)$.

For \ttt{lambda}=0 ridge regression reduces to OLS regression. By default (\ttt{form = 'auto'}), MVPA-Light dynamically switches between the primal and the dual form depending on whether $n$ is larger or smaller than $p$.

%%% -------- KERNEL RIDGE --------
\subsubsection{Kernel Ridge Regression}

Analogous to kernel classifiers (\prettyref{sec:kernel-methods}), a non-linear version of ridge regression can be developed by applying a non-linear transformation to the features. Let this transformation be represented by $\phi:\mathcal{X}\mapsto\mathcal{F}$, a map from input space to a Reproducing Kernel Hilbert Space, and $\Phi(\X) = [\phi(\x_1),\phi(\x_2),...,\phi(\x_n)]^\top$. The solution is given by replacing $\X$ by $\Phi(\X)$ in \prettyref{eq:ridge},

\begin{align}
\begin{split}
\label{eq:kernel_ridge_in_feature_space}
\w_\phi =\ & (\Phi(\X)^\top\Phi(\X) + \lambda\I)^{-1}\ \Phi(\X)^\top\y \quad\text{(primal solution)}\\
 =\ & \Phi(\X)^\top (\Phi(\X)\Phi(\X)^\top + \lambda\I_n)^{-1}\ \y \quad\text{(dual solution)}
\end{split}
\end{align}

Unfortunately, this solution is of limited practical use, since generally speaking the feature space is too high-dimensional to represent $\w_\phi$ and $\Phi(\X)$. However, the dual solution can be rewritten as follows. Let $\K = \Phi(\X)\,\Phi(\X)^\top$ be the kernel matrix with $\K_{ij} = k(\x_i, \x_j)$ for a kernel function $k$. Define the vector of dual weights $\al$ as

\begin{align}
\begin{split}
\label{eq:kernel_ridge}
\al = (\K + \lambda\I_n)^{-1}\ \y.
\end{split}
\end{align}

Then the predicted response to a test sample $\x$ can be rewritten in terms of kernel evaluations:

\begin{align}
\begin{split}
\label{eq:kernel_ridge_apply}
f(\x) = \w_\phi^\top\, \phi(\x) = \al^\top\Phi(\X)\,\phi(\x) = \sum_{i=1}^n \alpha_i\, k(\x_i, \x).
\end{split}
\end{align}


%%% ---------------------------------
%%% ----   PERFORMANCE METRICS   ----
%%% ---------------------------------
\subsection{Performance Metrics}\label{sec:metrics}

In most cases, the quantity of interest is not the raw model output but rather a metric that summarizes the performance of the classifier or regression model on test data. The desired metric can be specified by e.g. setting \ttt{cfg.metric = 'accuracy'} in any high-level function. Multiple metrics can be requested by providing a cell array, e.g. \ttt{cfg.metric = \{'accuracy', 'auc'\}}. Table \ref{tab:metrics} lists the metrics implemented in MVPA-Light. For a thorough discussion of classification metrics, refer to  \cite{Sokolova2009ATasks}.

\begin{table}[]
\begin{tabularx}{\textwidth}{lllX}
Task & Metric &  Range & Description   \\\hline
Classification & \ttt{'accuracy'}  &  [0,1] &  Fraction correctly predicted class labels.\\
 & \ttt{'auc'} & [0,1] & For two classes only. An alternative to classification accuracy that is more robust to imbalanced classes. Requires continuous classifier output (decision values or probabilities). 0.5 means chance-level performance and 1 means perfect separation of the classes.\\
 & \ttt{'confusion'} & [0,1] &  Confusion matrix. Rows corresponds to true class, columns  to predicted class. The $(i,j)$-th element gives the proportion of samples of class i that have been classified as class j.\\
 & \ttt{'dval'} & ($-\infty,+\infty$) & For two classes only. Average decision value, for each class separately.\\
 &\ttt{'f1'} & [0,1] & Combines precision (PR) and recall (R) into a single score using the harmonic average 2*PR*R / (PR+R).\\
&\ttt{'kappa'} & [-1, 1] & Cohen's kappa, a measure of inter-rater reliability.\\
& \ttt{'precision'} & [0,1] & TP / (TP + FP). Fraction of samples labeled as positive that actually belong to the positive class. For multi-class, it is calculated per class from the confusion matrix.\\
& \ttt{'recall'} & [0,1] & TP / (TP + FN). Fraction of positive samples that have been detected. For multi-class, it is calculated per class from the confusion matrix.\\
&\ttt{'tval'} & ($-\infty,+\infty$) & For two classes only. T-test statistic for the unequal sample size, equal variance case, based on decision values.\\
&\ttt{'none'} & ($-\infty,+\infty$) & Returns a cell array with the raw classifier outputs for all test sets.\\\hline
Regression & \ttt{'mae'}  & [$0,\infty$) &  Mean absolute error: $1/n \sum_{i=1}^n|y_i - \hat{y}_i|$.\\
& \ttt{'mse'}  & [$0,\infty$) &  Mean squared error: $1/n \sum_{i=1}^n(y_i - \hat{y}_i)^2$.\\
& \ttt{'r\_squared'} & ($-\infty,1$] &  $R^2$ coefficient representing the fraction of variance explained by the model.\\
\hline
\end{tabularx}
\caption{Metrics in MVPA-Light. Abbreviations for classification: TP = true positives, FP = false positives, FN = false negatives. Regression: $y$ = responses, $\hat{y}$ = model predictions.}
\label{tab:metrics}
\end{table}

If cross-validation is used then the metric is initially calculated for each test set in each repetition separately. It is then averaged across test sets and repetitions. Since the number of samples in a test set can vary across different folds, a proportionally weighted average is used whereby larger test sets get a larger weight.

%%% ---------------------------------
%%% ----   STATISTICS   ----
%%% ---------------------------------
\subsection{Statistical Analysis}\label{sec:statistics}

In neuroimaging experiments, establishing the statistical significance of a metric is often more important than maximizing the metric per se. Neuroimaging data is typically hierarchical: a study comprises many subjects, and each subject comprises many trials. To perform group analysis, a common approach is then to start with a level 1 (single-subject) analysis and calculate a classification or regression metric. At this stage, the samples consist of single trials for a particular subject. The metrics are then taken on to level 2 (group level). At this stage, each subject constitutes one sample  \citep{Mumford2007ModelingData}.
The function \ttt{mv\_statistics} implements both level 1 (single-subject) and level 2 (group level) statistical analysis. For level 1 analysis, the following tests are available:

\begin{itemize}
    \item Binomial test: uses a binomial distribution to calculate the p-value under the null hypothesis that classification accuracy is at chance. Requires classification accuracy as metric.
    \item Permutation test: non-parametric significance test. Creates a null distribution by shuffling the class labels or responses and repeating the multivariate analysis e.g. 1000 times.
    \item Cluster permutation test: an elegant solution to the multiple comparisons problems arising when MVPA is performed along multiple dimensions (e.g. for each time-frequency point). Uses the cluster statistic introduced in
    \cite{Maris2007}.
\end{itemize}

For level 2 analysis, a permutation test (with and without cluster correction) is available for within-subject and between-subjects designs. Note that no classification/regression is performed. The metrics that have been obtained in the level 1 analysis for each subject are simply subjected to a standard statistical test. In the \textit{within-subject} design, two different cases are considered. If pairs of values have been observed (e.g. mean decision values for class 1 and 2) they are tested for a significant difference across subjects. If only one value has been observed (e.g. AUC) it is tested against a given null value (e.g. 0.5). As test statistics, mean, t-test, or Wilcoxon signed-rank test can be used. To create a null distribution, data is permuted by randomly swapping the pairs of values or swapping the value and its null value. In \textit{between-subjects} design, subjects are partitioned into two different groups. The test statistic quantifies whether the metric differs between two groups. A null distribution is created by randomly assigning subjects to groups.

To illustrate this with an example, consider the 'faces vs houses' experiment. For the within-subject design, assume the mean decision values for houses and faces have been determined for each subject using cross-validation. A paired-samples t-test across subjects comparing the decision value for faces vs houses is used to calculate a t-statistic. A null distribution is created by randomly swapping face and house values for each subject and recomputing the statistic. For a between-subjects design, assume the experiment has also been carried out with a clinical group of Parkinson's patients and AUC values have been recorded for both groups. A Wilcoxon signed-rank test is used to compare the AUC for the two groups at each voxel. A null distribution is created by randomly assigning subjects to either the clinical or the control group.


\subsection{Custom Classifiers and Regression Models}

MVPA-Light can be extended with custom models. To this end, the appropriate train and test functions need to be implemented. Additionally, default hyperparameters need to be added to the function \ttt{mv\_get\_hyperparameter}. In the appendix, it is shown how to implement a prototype classifier that assigns a sample to the closest class centroid.

\subsection{LIBSVM and LIBLINEAR}\label{sec:libsvm}

LIBSVM \citep{Chang2011LIBSVM:Machines} and LIBLINEAR \citep{Fan2008} are two high-performance libraries for SVM, Support Vector Regression (SVR), and Logistic Regression. In order to use the libraries with MVPA-Light, the user needs to follow the installation instructions on the respective websites. In particular, the C-code needs to be compiled and added to the MATLAB path. In MVPA-Light, the models are denoted as \ttt{'libsvm'} and \ttt{'liblinear'}.

\subsection{FieldTrip Integration}

The FieldTrip \citep{Oostenveld2011} function \ttt{ft\_statistics\_mvpa} provides a direct interface between FieldTrip and MVPA-Light. In brief, the function calls MVPA-Light functions to carry out multivariate analysis, and then stores the results back into FieldTrip structs. To use MVPA-Light from high-level FieldTrip functions such as \ttt{ft\_timelockstatistics}, one has to set the parameter \ttt{cfg.method = 'mvpa'}. The interface is introduced in detail in a tutorial on the FieldTrip website \footnote{\ttt{http://www.fieldtriptoolbox.org/tutorial/mvpa\_light/}}.

%%% -------------------------
%%% ----  DEVELOPMENT    ----
%%% -------------------------
\subsection{Development}\label{sec:development}

Apart from the \ttt{main} branch intended for general use, the GitHub repository features a \ttt{devel} branch wherein new code is developed before committing into the main branch. Additionally, the branch features code that helps in maintaining the integrity of the toolbox. The \ttt{unittests/} subfolder features a unit testing framework for all models, optimization algorithms, high-level functions and some of the important utility functions. The unit tests make use of both the example EEG data, random noise, and simulated data. Unit testing can be triggered by executing the \ttt{run\_all\_unittests} function.

\subsection{Analysis of a MEEG Dataset}

To illustrate MVPA-Light on a real dataset, a multivariate analysis was conducted on a multi-subject, multi-modal face processing dataset wherein subjects viewed images of famous faces, familiar faces, or scrambled faces. See  \cite{Wakeman2014OpenfMRI,Wakeman2015ADataset} for a detailed description of the data. The dataset contains 16 subjects with EEG and MEG simultaneously recorded. The MEEG data was preprocessed using FieldTrip. It was low-pass filtered with a cut-off of 100 Hz and high-pass filtered using a FIR one-pass zero-phase filter with a cut-off of 0.1 Hz. A bandstop filter was applied at 50 Hz to suppress line noise. Subsequently, data was downsampled to 220 Hz and for each subject, the 6 separate runs were combined into a single dataset, yielding 880--889 trials per subject with roughly equal proportions for the three classes. All trials displaying famous faces were coded as class 1, familiar faces as class 2, and scrambled faces as class 3. MVPA was performed to investigate the following questions:

\begin{enumerate}
    \item ERP classification: \cite{Wakeman2015ADataset} found two prominent event-related components, a N170 and a sustained component roughly starting at 400 ms post-stimulus. Cross-validation with a multi-class classifier was used to investigate whether these components discriminate between the three classes.
    \item Time classification: Is there more discriminative information in MEG than in EEG? To answer this, classification across time was performed for three different channel sets, namely EEG only, MEG only, and EEG+MEG combined.
    \item Time-frequency classification: Is the discriminative information for famous vs scrambled faces confined to specific oscillatory frequencies and times? To answer this, time-frequency spectra were calculated for single trials and classification was performed at each time-frequency bin separately.
    \item Generalization: Are representations shared across time \citep{King2014} or frequency? To answer this, time generalization (time x time classification) was applied to the ERF data, and frequency generalization (frequency x frequency classification) was applied to the time-frequency data.
\end{enumerate}

MVPA was performed at the sensor level using a LDA classifier. All analyses were cross-validated using 5-fold or 10-fold cross-validation. Only the MEG channels were used as features except for analysis 2, where different sets of channels were compared. To assess statistical significance, the following tests were carried out:

\begin{itemize}
    \item Level 1 statistics. For each subject, the statistical significance of the time generalization (famous vs scrambled faces) was investigated. For illustrative purposes, the three statistical tests contained in MVPA-Light were compared: binomial, permutation, and cluster permutation tests. Permutation tests were based on 500 random permutations of the class labels. The cluster permutation test was corrected for multiple comparisons by using a cluster statistic, the other tests were uncorrected. For the cluster statistic, a critical value of 0.6 was chosen for classification accuracy. This analysis is reported only for the first subject.
    \item Level 2 statistics (across subjects). The AUC values obtained in the time-frequency classification analyses were statistically compared to a null value of 0.5 using cluster permutation tests based on a within-subject design.

\end{itemize}

\subsection{Analysis of a fMRI dataset}
To illustrate the application of MVPA-Light to fMRI data, another analysis was conducted using a block-design fMRI study. See \cite{Haxby2001} for a detailed description. The dataset was downloaded from \ttt{http://www.pymvpa.org/datadb/haxby2001.html}. The study investigates face and object representations in human ventral temporal cortex. It comprises 6 subjects with 12 runs per subject. In each run, subjects viewed greyscale images of 8 living and non-living object categories,  grouped in 24 s blocks separated by rest periods. Images were shown for 500 ms followed by a 1500 ms inter-stimulus interval. Full-brain fMRI data were recorded with a volume repetition time of 2.5 s. Hence, a stimulus block was covered by roughly 9 volumes. A zero-phase Butterworth high-pass filter with a cut-off frequency of 0.01 Hz was applied in order to remove slow drifts. No other preprocessing was performed. The following questions were addressed:

\begin{enumerate}
    \item Confusion matrix: Which image categories lead to similar brain activation patterns?
    \item Time classification: How does classification performance evolve across time following stimulus onset?
    \item Searchlight analysis: Which of the brain regions contain discriminative information that discerns between faces and houses?
\end{enumerate}

 Leave-one-run-out cross-validation was used to calculate classification performance. Multi-class LDA with 8 classes served as a classifier. For the searchlight analysis, binary LDA contrasting faces vs houses was used with AUC serving as metric. The searchlight consisted of a 3x3x3 cube of voxels that was centered on each target voxel. A level 2 cluster permutation test was computed on the AUC values against the null hypothesis that AUC equals 0.5.

\subsection{Benchmarking}

%In multi-dimensional neuroimaging data, a model needs to be trained on a single subject's data potentially thousands of times. This is exacerbated in permutation testing, when this process needs to be repeated hundreds or thousands of times.
Multivariate analyses can involve hundreds or even thousands of train/test iterations. Therefore, training time (the amount of time required to train a single model on data) is a relevant quantity when evaluating different model implementations. To benchmark MVPA-Light's models, their training time was compared to models in the MATLAB Statistics Toolbox as well as models in Python (Scikit Learn package) and R (different packages). The comparison to other MVPA toolboxes is of less relevance since they often rely on external packages such as LIBSVM and LIBLINEAR which are also available in MVPA-Light (this applies to e.g. DDTBOX, PRoNTo, TDT). The following three datasets were considered:

%A key variable to computational efficiency is training speed, that is, the amount of time needed to optimize the model parameters on training data.

\begin{itemize}
    \item MEG single-subjects. The \cite{Wakeman2015ADataset} dataset was used with the famous vs scrambled faces conditions, epoched in the range [-0.2, 1] s. Data dimensions were 585--592 trials per subject, 306 channels, and 265 time points. MVPA was performed for every subject and every time point separately, using channels as features.
    \item MEG super-subject. Trials of all subjects in the MEG single-subjects data were concatenated to form a single 'super-subject' comprising 9,421 trials, 306 channels, and 265 time points. MVPA was performed for every time point separately, using channels as features.
    \item fMRI. For each subject in the \cite{Haxby2001} data, all voxels with a non-zero signal were concatenated to a single feature vector. The time dimension was dropped, different time points within a trial were simply considered as different samples. The two classes 'face' and 'house' were considered, yielding a data matrix of 216 samples (198 samples for subject 5) and between 163,665 and 163,839 voxels per subject. MVPA was performed for every subject separately, using voxels as features.
\end{itemize}

The MEG single-subjects dataset is of standard size for neuroimaging data and thus serves as a benchmark for ordinary operation. The other two datasets are intended to test the computational limits of the models by using either a large number of trials (MEG super-subject) or a large number of features (fMRI). For the single-subjects dataset, classification performance was measured in addition to training time.
To be as unbiased as possible, hyperparameters were mostly unchanged except when a change made the models more comparable across toolboxes (e.g. setting the same regularization value). No hyperparameter tuning was performed in order to quantify pure training time.

The MVPA-Light models were compared to LIBSVM,  LIBLINEAR, and MATLAB 2019a's fitcdiscr (LDA), lassoglm (LogReg), fitcnb (Naive Bayes), fitcsvm (SVM), ridge, and fitrsvm (SVR). Python and R-based toolboxes were installed in virtual environments using Anaconda 4.7.12. Scikit Learn 0.21.2 was used together with Python 3.7.3. R version 3.6.1 was used with packages MASS (LDA), glmnet (LogReg and Ridge), e1071 (Naive Bayes, SVM, SVR), and listdtr (Kernel Ridge).

For the single-subject data, the timing results were averaged across subjects. Then for both the single-subject and the super-subject, mean and standard deviation was calculated across time points. For the fMRI data, mean and standard deviation was calculated across subjects. All analyses were conducted after a fresh restart of a desktop computer with networking disabled. The computer had an Intel Core i7-6700 @ 3.40 GHz x 8 CPU with 64 GB RAM running on Ubuntu 18.04. All scripts are available in the accompanying GitHub repository\footnote{https://github.com/treder/MVPA-Light-Paper}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% -------- RESULTS --------   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results}

\subsubsection{MEEG Data}
Figure \ref{fig:results} depicts the results of the MVPA, averaged across subjects. Errorbars depict standard error across subjects.

\textit{ERP classification} (Figure \ref{fig:results}a). The bar graph shows that for both the N170 and the sustained ERP component classification accuracy is significantly above the chance level of 33\%. Accuracy can be broken down into confusion matrices that show which combinations of classes get misclassified ('confused'). For both N170 and the sustained ERP component, the highest accuracy is obtained for the scrambled images (0.63 and 0.78). Moreover,  misclassification (off-diagonal elements) is most prominent for the famous and unfamiliar faces. This is not surprising since both types of images are identical in terms of low-level features and both show actual faces, in contrast to the scrambled images.

\textit{Time classification} (Figure \ref{fig:results}b). The classes are not discriminable prior to the occurrence of the N170. A classification peak at the time of the N170 can be seen for all channel sets. At this stage, the AUC values diverge, with EEG yielding a significantly lower AUC. Combining EEG+MEG seems to yield a slightly higher performance than MEG alone.

\textit{Time-frequency classification} (Figure \ref{fig:results}c). For famous vs scrambled faces, peak performance is reached in the delta frequency band at a latency between 0.2 and 0.4 s. For famous vs unfamiliar faces, peak performance is attained in the latter half of the trial (0.5 - 1 s) in the theta and alpha frequency bands.

\textit{Generalization} (Figure \ref{fig:results}d). The first plot depicts AUC (color-coded) as a function of training time (y-axis) and testing time (x-axis). There is evidence for widespread time generalization for famous vs scrambled faces starting about at the time of the N170 peak and covering most of the remaining trial. In particular, there is generalization between the N170 and the later sustained component (horizontal and vertical lines emanating at 0.17 s), suggesting some correlation between the spatial pattern of the N170 and the sustained component.
The second plot depicts AUC as a function of frequency. There is some generalization in the theta band (lower-left corner), the alpha band, and the lower beta band (16--22 Hz). Also, when the classifier is trained in the beta band, classification performance partially generalizes to the alpha band. However, the overall performance is low when compared to the time-locked data.

\textit{Level 2 statistics} (Figure \ref{fig:results}e). Group statistical analysis  based on the time-frequency classification data in the panel above. Images depict AUC values masked by significance (deep blue = not significant). For the famous vs scrambled faces classification, a large cluster spanning the whole trial and especially the low frequency bands is evident. For the famous vs unfamiliar faces condition, there is a significant cluster corresponding to large AUC values evident after 0.5 s and confined to the lower frequency range.

\textit{Level 1 statistics} (Figure \ref{fig:results}f). Level 1 statistical analysis based on the time generalization data in the panel above, shown exemplarily for subject 1. Images depict the AUC values masked by significance. Both uncorrected tests (binomial and permutation test) exhibit spurious effects even at pre-stimulus time. Most of these spurious effects disappear under the cluster permutation test.

\subsubsection{fMRI Data}

Figure \ref{fig:fmri_results} depicts the results of the MVPA on the fMRI data, averaged across subjects.

\textit{Confusion matrix} (Figure \ref{fig:fmri_results}a). A mask provided with the data was applied to select voxels from ventral temporal areas.
A high overall performance is observed for LDA with 8 classes. Misclassifications tend to be confined to general semantic categories. For instance, misclassified faces tend to be labeled as cats (both living objects), whereas misclassified non-living objects tend to be labeled as other non-living objects. This indicates that there are shared representations for images from the same general category.

\textit{Time classification} (Figure \ref{fig:fmri_results}b). Although all ROIs and time points yield performances above the chance level of 12.5\%, the ventral temporal area (which comprises both face and house responsive voxels) yields the best performance. For the latter, classification performance peaks at about 5 s after stimulus onset.

\textit{Searchlight analysis} (Figure \ref{fig:fmri_results}c). AUC values averaged across subjects are depicted. The AUCs are masked by the significant cluster ($p < .01$) and overlayed on an averaged anatomical MRI. Although the cluster is large, high values $>0.8$ are predominantly found in dorsal and ventral visual areas including the paraphippocampal place area and the fusiform area, nicely dovetailing with the original findings of \cite{Haxby2001}.

\subsubsection{Benchmarking}

Figure \ref{fig:benchmarking} depicts ERP classification accuracy across time on the MEG single-subjects data for different classifiers and different toolboxes, averaged across subjects. Except for the MATLAB classifiers, results are nearly identical  for all implementations of LDA, LogReg, and linear SVM, with a peak performance of about 75\%. Lower performance is evident for Naive Bayes, but consistently so across different implementations. For SVM with a RBF kernel, the best performance is obtained in R, followed MATLAB, with both MVPA-Light and Scikit Learn performing worse. Since no hyperparameter tuning was performed, the latter result is most likely due to differences in the default hyperparameters. 

Tables \ref{tab:classification}, \ref{tab:regression} show the timing results for different classifiers and regression models. These results are discussed model by model:

\textit{LDA}. The MVPA-Light implementation consistently outperforms other implementations in terms of training time, in some cases by orders of magnitude. For the fMRI dataset, it is almost 100 times faster than Scikit Learn, whereas MATLAB and R both run out of memory. It is worth noting that a shrinkage value of 0.01 was applied for the MVPA-Light and MATLAB implementations. For R, low performance was achieved with rda (regularized LDA), so the standard unregularized LDA was used. For Scikit Learn, the default solver does not allow for shrinkage so no shrinkage was applied.

\textit{LogReg}. The MVPA-Light implementation of Logistic Regression outperforms the competitors for the MEG single-subjects data. It is outperformed by the R implementation for the MEG super-subject. For the fMRI data, it causes an out of memory error and the best performing model is LIBLINEAR.

\textit{Naive Bayes}. The MVPA-Light implementation consistently outperforms other implementations, in some cases by orders of magnitude. Scikit Learn is consistently second best, followed by R and MATLAB.

\textit{SVM}. For linear SVM, LIBLINEAR yields the best training speed except for the fMRI data, where MVPA-Light performs best. For RBF kernels, MVPA-Light's SVM consistently outperforms the competitors, closely followed by MATLAB's fitcsvm. Significant differences are obtained for different toolboxes, with R being the slowest in many cases. The good performance of MVPA-Light's SVM may appear surprising at first glance, given some of its contenders run using C code. First, MVPA-Light uses a large tolerance value; this implies that its algorithm might perform fewer iterations than LIBSVM, although this has not been investigated. If this is the case, it does not seem to be detrimental to classification performance, as Figure \ref{fig:benchmarking} illustrates. Second, the advantages of LIBSVM might not play out during a single training iteration. It has an integrated cross-validation procedure, which is likely to be substantially faster than cross-validation using MVPA-Light, although this has not been investigated either.

\textit{Ridge and Kernel Ridge}. MVPA-Light's models lead the field except for the fMRI data, where Scikit Learn's kernel ridge outperforms MVPA-Light. No results are available for R's krr model; it does not appear to have an interface for fixing hyperparameters and instead performs an expensive search using leave-one-out cross-validation, so it was omitted.

\textit{SVR}. MVPA-Light exclusively relies on LIBSVM for SVR, which leads the field except for one case, in which it closely trails the MATLAB implementation. Overall, R yields the slowest implementation.

\begin{table}
\footnotesize
\begin{tabular}{ |p{1.1cm}|p{1.7cm}|p{2cm}|p{2.1cm}|p{2cm}|p{2.1cm}|p{2cm}|}
 \hline
 Dataset & Toolbox & \multicolumn{5}{c}{Classifier}\\ \hline
\multicolumn{2}{|c|}{}  & LDA & LogReg & Naive Bayes & SVM (linear) & SVM (RBF)\\
 \hline
\multirow{6}{1.3cm}{MEG single-subjects} & MVPA-Light & $\mathbf{0.003}\pm 0.0001$ & $\mathbf{0.0097}\pm0.0005$ & $\mathbf{0.001}\pm0.00004$ & $0.07\pm0.002$ & $\mathbf{0.02}\pm0.0001$\\
\cline{2-7}
  & LIBLINEAR & - & $0.014\pm0.0009(p)$ $0.035\pm0.001 (d)$ & - & $\mathbf{0.023}\pm0.002(p)$ $0.231\pm0.02(d)$ & -\\
\cline{2-7}
  & LIBSVM & - & - & - & $0.098\pm0.01$ & $0.125\pm0.001$\\
\cline{2-7}
  & MATLAB & $0.026\pm0.0008$ & $0.03\pm0.006$ & $0.05\pm0.0001$ & $0.041\pm0.004$ & $0.023\pm0.0004$\\
\cline{2-7}
   & Scikit Learn & $0.097\pm0.0006$ & $0.1\pm0.005$ & $0.007\pm0.0001$ & $0.37\pm0.052$ & $0.45\pm0.032$\\
\cline{2-7}
  & R & $0.084\pm 0.0003$ & $0.013\pm0.002$ & $0.04\pm0.0001$ & $0.71\pm0.113$ & $0.41\pm0.026$ \\\hline
 \hline
 \multirow{6}{1.3cm}{MEG super-subject} & MVPA-Light & $\mathbf{0.026}\pm0.0028$ & $0.437\pm0.0062$ & $\mathbf{0.015}\pm0.0001$ & $10.122\pm1.05$ & $\mathbf{5.369}\pm0.033$\\
\cline{2-7}
  & LIBLINEAR & - & $0.732\pm0.068(p)$ $0.998\pm0.063(d)$& - & $\mathbf{1.338}\pm0.168(p)$ $6.29\pm0.519(d)$& -\\
\cline{2-7}
  & LIBSVM & - & - & - & $42.089\pm4.188$ & $37.941\pm0.404$\\
\cline{2-7}
  & MATLAB & $0.149\pm0.002$ & $0.279\pm0.137$ & $0.231\pm0.027$ & $20.98\pm1.78$ & $11.65\pm0.217$\\
\cline{2-7}
   & Scikit Learn & $0.596\pm0.017$ & $2.065\pm0.109$ & $0.09\pm0.001$ & $32.19\pm2.07$& $34.56\pm0.38$\\
\cline{2-7}
  & R & $0.84\pm.004$ & $\mathbf{0.159}\pm0.018$ & $0.144\pm.0006$ & $1123.16\pm27.39$ & $123.31\pm9.38$\\\hline
  \hline
\multirow{6}{1.3cm}{fMRI} & MVPA-Light & $\mathbf{0.293}\pm0.0078$ & OOM & $\mathbf{0.309}\pm0.011$ & $\mathbf{0.182}\pm0.0086$ & $\mathbf{2.064}\pm0.235$\\
\cline{2-7}
  & LIBLINEAR & - & $\mathbf{4.008}\pm0.627(p)$ $6.689\pm1.018(d)$ & - & $2.235\pm0.218(p)$ $6.125\pm0.995(d)$ & - \\
\cline{2-7}
  & LIBSVM & - & - & - & $11.79\pm0.787$ & $11.88\pm0.822$ \\
\cline{2-7}
  & MATLAB & OOM & $23.79\pm4.008$ & $357.49\pm2.205$ & $5.053\pm0.325$ & $4.845\pm0.308$\\
\cline{2-7}
   & Scikit Learn & $24.45\pm1.1$ & $20.68\pm4.24$ & $2.86\pm0.06$ & $10.46\pm0.59$ & $9.15\pm0.59$\\
\cline{2-7}
  & R & OOM & $7.1\pm1.13$ & $18.48\pm0.35$ & $39.67\pm1.98$ & $43.3\pm2.18$ \\\hline
\end{tabular}
\caption{Benchmarking results: Mean training time and standard deviation in seconds for different classifiers. For each combination of dataset and classifier, the fastest model is marked in bold. Abbreviations: OOM = out of memory error, (p) = primal form, (d) = dual form.}
\label{tab:classification}
\end{table}

% --- REGRESSION ---
\begin{table}
\footnotesize
\begin{tabular}{ |p{1.1cm}|p{1.8cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
 \hline
 Dataset & Toolbox  & \multicolumn{4}{c|}{Regression model}\\ \hline
\multicolumn{2}{|c|}{}  & Ridge & Kernel Ridge & SVR (linear) & SVR (RBF)\\
 \hline
\multirow{4}{1.3cm}{MEG single-subjects} & MVPA-Light  & $\mathbf{0.0016}\pm0.00006$ & $\mathbf{0.019}\pm0.0001$ & - & -\\
\cline{2-6}
  & LIBSVM & - &  - & $0.02\pm0.001$ & $\mathbf{0.0041}\pm0.0002$ \\
\cline{2-6}
  & MATLAB & $0.0061\pm0.0002$ & - & $\mathbf{0.018}\pm0.037$ & $0.023\pm0.0005$\\   %$\pm$
\cline{2-6}
  & Scikit Learn & $0.0069\pm0.0003$ & $0.023\pm0.003$ & $0.654\pm0.0647$ & $0.481\pm0.02$\\
\cline{2-6}
  & R & $0.055\pm0.0027$ & - & $1.59\pm0.094$ & $0.43\pm0.002$ \\\hline
 \hline
 \multirow{4}{1.3cm}{MEG super-subject} & MVPA-Light & $\mathbf{0.015}\pm0.001$ & $\mathbf{7.38}\pm0.023$ & - & -\\
\cline{2-6}
  & LIBSVM & - &  - & $\mathbf{0.653}\pm0.038$ & $\mathbf{0.121}\pm0.014$ \\
\cline{2-6}
  & MATLAB & $0.186\pm0.007$ & - & $6.931\pm0.237$ & $9.9798\pm0.239$\\
\cline{2-6}
  & Scikit Learn & $0.062\pm0.005$ & $14.51\pm0.21$ & $3.213\pm0.394$ & $31.61\pm1.51$\\
\cline{2-6}
  & R & $0.547\pm0.0079$ & - & $465.08\pm49.83$ & $151.66\pm26.76$\\\hline
  \hline
\multirow{4}{1.3cm}{fMRI} & MVPA-Light & $\mathbf{0.165}\pm0.0042$ & $2.026\pm0.256$ & - & -\\
\cline{2-6}
  & LIBSVM & - &  - & $\mathbf{4.334}\pm1.48$ & $\mathbf{2.819}\pm0.0412$ \\
\cline{2-6}
  & MATLAB & OOM & - & $4.545\pm0.353$  & $4.563\pm0.284$\\
\cline{2-6}
  & Scikit Learn & $0.638\pm0.022$ & $\mathbf{0.476}\pm0.01$ & $16.138\pm3.64$ & $9.999\pm0.59$\\
\cline{2-6}
  & R & $7.503\pm0.593$ & - & $37.211\pm2.056$ & $41.037\pm2.298$\\\hline
\end{tabular}
\caption{Benchmarking results: Mean training time and standard deviation in seconds for different regression models. For each combination of dataset and model, the fastest model is marked in bold. Abbreviations: OOM = out of memory error, (p) = primal form, (d) = dual form.}

\label{tab:regression}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% -------- DISCUSSION --------   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

MVPA-Light offers a suite of classifiers, regression models and metrics for multivariate pattern analysis. A high-level interface facilitates common MVPA tasks such as cross-validated classification across time, generalization, and searchlight analysis. The toolbox supports hyperparameter tuning, pre-computed kernels, and  statistical significance testing of the MVPA results.
%Since the toolbox is entirely statistical (i.e. it just operates on data matrices), it can easily be combined with MATLAB-based neuroimaging toolboxes.

% Preprocessing
MVPA-Light also provides a nested preprocessing pipeline that applies operations to training and test sets separately. Among others, it features over- and undersampling, PCA, and scaling operations. It also includes an averaging approach wherein samples are assigned to groups and then averaged in order to increase signal-to-noise ratio. For linear classifiers, this approach has been explored by \citep{Cichy2015,Cichy2017MultivariateSpace}. Recently, it has been generalized to non-linear kernel methods \citep{Treder2018}. Either approach can be used in the toolbox by adding the operation \ttt{average\_samples} or \ttt{average\_kernel} to the preprocessing pipeline.
% Wakeman & Henson and Haxby analysis
To showcase some of its features, analyses of an MEEG \citep{Wakeman2015ADataset} and an fMRI \citep{Haxby2001} dataset are reported. The results illustrate some ways in which the toolbox can aid in quantifying the similarity of representations, measuring the information content, localizing discriminative information in the time-frequency plane, highlighting shared representations across different time points or frequencies, and establishing statistical significance.

% Benchmarking
A benchmarking analysis was conducted in order to compare MVPA-Light (including LIBSVM and LIBLINEAR) to models provided in the MATLAB Statistics Toolbox, various R packages, and Scikit Learn  for Python. While classification performance is largely consistent across different platforms, training time varies considerably. The MVPA-Light implementations of LDA, Naive Bayes, and Ridge Regression consistently outperform  their competitors, in some cases by orders of magnitude. For Logistic Regression and SVM, the MVPA-Light implementations and LIBLINEAR lead the field. In all but one case, MVPA-Light's classifiers are faster than the contenders in MATLAB, R, and Scikit Learn. Overall, the fastest classifier is MVPA-Light's LDA and the fastest regression model is MVPA-Light's Ridge Regression. Partially, the success of MVPA-Light is due to specialization: MVPA-Light models tend to have fewer hyperparameters than other models, and MVPA-Light features separate optimized implementations for binary LDA and multi-class LDA, whereas the other toolboxes have a single implementation. Furthermore, MVPA-Light's LDA and Ridge Regression dynamically switch between primal and dual form. This can increase computational efficiency especially when dealing with a large dataset.

The benchmarking results should not be interpreted as final verdicts on the respective toolboxes.
 Undoubtedly, training speed can be improved by finding an optimal set of hyperparameters for a model. For instance, increasing regularization tends to lead to smoother loss surfaces and often faster convergence for gradient descent algorithms. The strategy for the present analysis was to change default parameters minimally and, if so, only in order to increase comparability e.g. by setting a regularization parameter to a common value.
 Although MVPA-Light will likely perform well in other situations, too, the present results are mostly indicative of \textit{default performance}, obtained with minimal user interference. This is a relevant measure since it is our belief that the burden of hyperparameter selection should be taken off the user as much as possible.


%Implementations of the same model in different software packages can use different solvers, different regularization techniques, and have a different set of default hyperparameters and capabilities that may cause additional overhead during optimization.
%These results should not be interpreted as final verdicts on the corresponding models.

% % Statistics
% Second-level statistical analyses are not currently covered by the toolbox. Such statistics can be obtained by combining e.g. the cluster permutation statistics framework in FieldTrip \citep{Maris2007} with MVPA-Light. For single subjects, a binomial test for accuracy is implemented in the function \ttt{mv\_statistics}. For the future, it is planned to include a second-level statistical framework based on cluster permutation tests directly in the toolbox. Since permutation tests are computationally expensive, the framework is planned to include recent developments such as a fast permutation scheme for LDA and kernel FDA \citep{Treder2019DirectFDA}.

\subsection{Setting Up a MVPA Pipeline}

If one is spoilt for choice, selecting  a model, metrics, and preprocessing steps can be challenging. This section offers practical advice in this regard. Such recommendations tend to be subjective to some extent, hence users are encouraged to perform their own MVPA experiments and compare different models, hyperparameter settings etc. To prevent a statistical bias, extensive experiments should not be performed on the dataset at hand. Instead, a similar dataset e.g. recorded using the same hardware with a similar paradigm can be used for experimentation.\\

\subsection{Preprocessing the Data} 

Although MVPA can be applied to raw data, this may negatively affect performance, so data has ideally been cleaned and corrupted trials have been rejected. It is useful to normalize the data for numerical stability by e.g. z-scoring across trials such that each feature has mean = 0 and standard deviation = 1. This is particularly important for Logistic Regression which uses the exponential function. It also applies to LDA and kernel methods because lack of normalization can lead to results being dominated by the features with the largest scaling. Generally speaking, preprocessing operations should be nested in the cross-validation loop i.e. performed on the training set first and then applied to the test set. The \ttt{cfg.preprocess} option serves this purpose. In some cases such as demeaning, it may be admissible to perform the operation globally on the whole dataset, but one then needs to assure that there is no information leakage from the test set that could bias the results. The same argumentation applies to unsupervised techniques such as PCA. Any preprocessing steps involving the class labels, such as CSP \citep{Blankertz2008a}, also need to be nested. Furthermore, for kernel methods, computation can be speeded up by precomputing the kernel matrix using \ttt{compute\_kernel\_matrix}, although this approach does not work when generalization is required.

\subsection{Choosing a Classifier} 

Linear classifiers perform well in a large variety of tasks. LDA is a good default model, since it is fast and robust thanks to regularization \citep{Blankertz2011}. Logistic Regression and linear SVM are more resilient to outliers than LDA, so may be preferred for noisy or strongly non-Gaussian data. Logistic Regression has a hyperparameter-free regularization by default, hence it is more user-friendly than SVM which requires setting the hyperparameter $c$. Naive Bayes should only be used after the features have been decorrelated using PCA or ICA. For non-linear problems, kernel FDA or SVM can be used. Again, SVM requires $c$ to be set, whereas for kernel FDA the default regularization often works well. Regarding the choice of a kernel, the RBF kernel is adequate for most classification tasks, but its hyperparameter \ttt{gamma} determining the kernel width might require tuning. If maximizing classification accuracy is vital, it is worth to try an ensemble of classifiers.

\subsection{Choosing a Regression Model}

Ridge regression tends to perform well on a variety of tasks. If the data is noisy, linear Support Vector Regression (SVR) using LIBLINEAR can be applied. If the problem is  non-linear, either kernel ridge or kernel SVR using LIBSVM with a RBF kernel is recommended.

\subsection{Metrics}

The most common classification metric is accuracy. For multi-class problems, it is useful to complement it with a confusion matrix. For two classes, AUC is a good alternative to accuracy since it is more robust to class imbalances and invariant to shifts of the classifier threshold. When the roles of the classes are asymmetric (e.g. patients vs controls), it is useful to report precision and recall along with their harmonic mean (F1 score). If in doubt, report multiple metrics.

\subsection{Cross-Validation}

Classification and regression metrics should be cross-validated. Unless the number of samples is very small, leave-one-out cross-validation should be avoided because it suffers from a large bias; instead, use 5-fold or 10-fold cross-validation  \citep{James2013}. Since samples are randomly assigned to folds, repeating the cross-validation is recommended to get a more stable estimate.

\subsection{Conclusion}

MVPA-Light is a comprehensive toolbox for multivariate pattern analysis. Its models perform competitively compared to other implementations. Future development of MVPA-Light will include additional feature extraction techniques for oscillations, such as Common Spatial Patterns \citep{Blankertz2008a} and the Riemannian geometry approach \citep{Barachant2013}, and further computational improvements, such as efficient permutation testing for LDA/KFDA \citep{Treder2019DirectFDA} and faster calculation of the regularization path for SVM \citep{Hastie2004TheMachine}.

\section*{Conflict of Interest Statement}
%All financial, commercial or other relationships that might be perceived by the academic community as representing a potential conflict of interest must be disclosed. If no such relationship exists, authors will be asked to confirm the following statement:

The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.

\section*{Author Contributions}

MT developed the toolbox, performed all analyses and authored the manuscript.

\section*{Funding}
NA

\section*{Acknowledgments}
I would like to thank colleagues from the Psychology department at University of Birmingham for advice and early adaptation of the toolbox, Jan-Mathijs Schoffelen and Sophie Arana for their efforts towards integrating it into FieldTrip, and Hong-Viet Ngo and the  reviewers for insightful comments on the manuscript. Many thanks to all contributors to the GitHub repository
\footnote{\ttt{https://github.com/treder/MVPA-Light/graphs/contributors}}.


\section*{Data Availability Statement}
The MEEG dataset can be found in the OpenNeuro repository (https://openneuro.org/datasets/ds000117/versions/1.0.3). The fMRI dataset can be found on the PyMVPA website (http://www.pymvpa.org/datadb/haxby2001.html). Scripts and figures used in this paper are available in the accompanying GitHub repository (github.com/treder/MVPA-Light-Paper).
% Please see the availability of data guidelines for more information, at https://www.frontiersin.org/about/author-guidelines#AvailabilityofData

\bibliographystyle{frontiersinSCNS_ENG_HUMS} % for Science, Engineering and Humanities and Social Sciences articles, for Humanities and Social Sciences articles please include page numbers in the in-text citations
%\bibliographystyle{frontiersinHLTH&FPHY} % for Health, Physics and Mathematics articles
\bibliography{references}

%%% Make sure to upload the bib file along with the tex file and PDF
%%% Please see the test.bib file for some examples of references

\section*{Figure captions}

%%% Please be aware that for original research articles we only permit a combined number of 15 figures and tables, one figure with multiple subfigures will count as only one figure.
%%% Use this if adding the figures directly in the mansucript, if so, please remember to also upload the files when submitting your article
%%% There is no need for adding the file termination, as long as you indicate where the file is saved. In the examples below the files (logo1.eps and logos.eps) are in the Frontiers LaTeX folder
%%% If using *.tif files convert them to .jpg or .png
%%%  NB logo1.eps is required in the path in order to correctly compile front page header %%%

% \begin{figure}[ht!]
% \centering\includegraphics[width=.6\linewidth]{X_clabel_cfg}
% \caption{The three most commonly used input arguments in MVPA-Light. Colors are used to represent the two classes 1 (turquoise) and 2 (red) for illustrative purposes. Note that \textbf{X} can be three-dimensional (e.g. if it also has a time dimension) or even higher-dimensional (e.g. time-frequency data).}\label{fig:X}
% \end{figure}

\begin{figure}[ht!]
\centering\includegraphics[width=\linewidth]{fig1_MVPA_structure}
\caption{Structure of MVPA-Light.}\label{fig:structure}
\end{figure}

\begin{figure}[ht!]
\centering\includegraphics[width=\linewidth]{overview_classifiers}
\caption{Overview of the available classifiers. Dots represent samples, color indicates the class. \textit{LDA}: different classes are assumed to have the same covariance matrix, indicated by the ellipsoids. \textit{Gaussian Naive Bayes}: features are conditionally independent, yielding diagonal covariance matrices. \textit{Logistic regression}: a sigmoid function (curved plane) is fit to directly model class probabilities. \textit{SVM}: a hyperplane (solid line) is fit such that the margin (distance from hyperplane to closest sample; indicated by dotted lines) is maximized. \textit{Ensemble}: multiple classifiers are trained on subsets of the data. In this example, their hyperplanes partition the data into spaces belonging to classes 1 and 2. After applying all classifiers to a new data point and collecting their 'votes', the class receiving most votes is selected. \textit{Kernel methods}: in this example the optimal decision boundary is circular (circle), hence the data is not linearly separable. After projection into a high-dimensional feature space using a map $\phi$, the data becomes linearly separable (solid line) and a linear classifier such as SVM or LDA can be successfully applied in this space.}\label{fig:classifiers}
\end{figure}

\begin{figure}[ht!]
\centering\includegraphics[width=\linewidth]{MVPA_results}
\caption{Results for the classification analysis of the \cite{Wakeman2015ADataset} MEEG data. a) Multi-class classification (famous vs unfamiliar vs scrambled faces) of N170 and sustained ERP component. b) AUC is plotted as a function of time for famous vs scrambled images. The classification was performed using three different channel sets: EEG  only, MEG only, and EEG+MEG combined. c) Binary classification (famous vs scrambled and famous vs unfamiliar) for time-frequency data. AUC is plotted as a function of both time and frequency. The AUC values are color-coded. d) Time x time generalization and frequency x frequency generalization using a binary classifier (famous vs scrambled). e) Level 2 statistical analysis of the time-frequency classification. f) Level 1 statistical analysis of the time x time generalization, shown exemplarily for subject 1. }\label{fig:results}
\end{figure}

\begin{figure}[ht!]
\centering\includegraphics[width=\linewidth]{fmri_results}
\caption{Results for the classification analysis of the \cite{Haxby2001} fMRI data. a) Confusion matrix for multi-class (8 classes) classification based on voxels in the ventral temporal area, averaged across subjects. b) Multi-class (8 classes) classification accuracy was calculated for each time point following stimulus onset. Lines depict  means across subjects, shaded areas correspond to standard error. Masks were used to select voxels in the ventral temporal area (yellow line), voxels responsive to faces (blue), or voxels responsive to houses (red). c) Cluster permutation test results based on a searchlight analysis using a binary classifier (faces vs houses). Red spots represent AUC values superimposed on axial slices of the averaged structural MRI. All depicted AUC values correspond to the significant cluster; other AUC values have been masked out. }\label{fig:fmri_results}
\end{figure}

\begin{figure}[ht!]
\centering\includegraphics[width=\linewidth]{benchmark_classification}
\caption{Mean ERP classification accuracy for the benchmarking analysis using the MEG single-subjects data (averaged across subjects). MVPA-Light is depicted as a solid black line.}\label{fig:benchmarking}
\end{figure}

%%% If you are submitting a figure with subfigures please combine these into one image file with part labels integrated.
%%% If you don't add the figures in the LaTeX files, please upload them when submitting the article.
%%% Frontiers will add the figures at the end of the provisional pdf automatically
%%% The use of LaTeX coding to draw Diagrams/Figures/Structures should be avoided. They should be external callouts including graphics.
\end{document}
