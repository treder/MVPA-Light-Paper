%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This is just an example/guide for you to refer to when submitting manuscripts to Frontiers, it is not mandatory to use Frontiers .cls files nor frontiers.tex  %
% This will only generate the Manuscript, the final article will be typeset by Frontiers after acceptance.   
%                                              %
%                                                                                                                                                         %
% When submitting your files, remember to upload this *tex file, the pdf generated with it, the *bib file (if bibliography is not within the *tex) and all the figures.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Version 3.4 Generated 2018/06/15 %%%
%%% You will need to have the following packages installed: datetime, fmtcount, etoolbox, fcprefix, which are normally inlcuded in WinEdt. %%%
%%% In http://www.ctan.org/ you can find the packages and how to install them, if necessary. %%%
%%%  NB logo1.jpg is required in the path in order to correctly compile front page header %%%

\documentclass[utf8]{frontiersSCNS} % for Science, Engineering and Humanities and Social Sciences articles
%\documentclass[utf8]{frontiersHLTH} % for Health articles
%\documentclass[utf8]{frontiersFPHY} % for Physics and Applied Mathematics and Statistics articles

%\setcitestyle{square} % for Physics and Applied Mathematics and Statistics articles
%\usepackage{url,hyperref,lineno,microtype,subcaption}
\usepackage{url,hyperref,lineno,microtype}
\usepackage[onehalfspacing]{setspace}

\linenumbers


% Leave a blank line between paragraphs instead of using \\


\def\keyFont{\fontsize{8}{11}\helveticabold }
\def\firstAuthorLast{Treder} %use et al only if is more than 1 author
\def\Authors{Matthias S. Treder}
% Affiliations should be keyed to the author's name with superscript numbers and be listed as follows: Laboratory, Institute, Department, Organization, City, State abbreviation (USA, Canada, Australia), and Country (without detailed address information such as city zip codes or street names).
% If one of the authors has a change of address, list the new address below the correspondence details using a superscript symbol and use the same symbol to indicate the author in the author list.
\def\Address{School of Computer Science \& Informatics, Cardiff University, Cardiff, UK}
% The Corresponding Author should be marked with an asterisk
% Provide the exact contact address (this time including street name and city zip code) and email of the corresponding author
\def\corrAuthor{Corresponding Author}

\def\corrEmail{trederm@cardiff.ac.uk}

%%% --- added by Matthias ---
\usepackage{tabularx}  % controls the table width
\usepackage{multirow}
\usepackage{prettyref}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{bbm}    % for 1 as a vector

\newtheorem{theorem}{Theorem}

% Math symbols
\newcommand{\al}{\boldsymbol{\alpha}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\mf}[2]{m_{#1#2}}
% \newcommand{\mf}[2]{m_#1^{(#2)}}
\newcommand{\mbar}{\overline{\m}}
\newcommand{\mm}[1]{\m_{#1}}
\newcommand{\sig}[2]{\sigma_{#1#2}^2}
% \newcommand{\sig}[2]{(\sigma^2)_#1^{(#2)}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\x}{\mathbf{x}}
% \newcommand{\xf}[1]{\mathbf{x}^{(#1)}} % x features
\newcommand{\xf}[1]{x^{(#1)}} % x features
\newcommand{\y}{\mathbf{y}}
\newcommand{\cov}{\text{cov}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\E}{\mathbb{E}}   % expectation
\newcommand{\G}{\mathbf{G}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\K}{\mathbf{K}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\N}{\mathcal{N}}   % normal distribution
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\S}{\mathbf{S}}
\newcommand{\Tr}[1]{\text{Tr}(#1)}
\newcommand{\W}{\mathbf{W}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}

\newcommand{\ttt}[1]{\texttt{#1}}

\newrefformat{fig}{Figure \ref{#1}}
\newrefformat{tab}{Table \ref{#1}}
\newrefformat{eq}{Eq. (\ref{#1})}
\newrefformat{app}{Appendix \ref{#1}}
\newrefformat{sec}{Section \ref{#1}}
\newrefformat{lemma}{Lemma \ref{#1}}
\newrefformat{theorem}{Theorem \ref{#1}}
\newrefformat{assumption}{Assumption \ref{#1}}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{red}{\textbf{todo} #1}}

\graphicspath{{../figures/}}


\begin{document}
\onecolumn
\firstpage{1}

%\title[MVPA-Light]{MVPA-Light: out-of-the-box classification of neuroimaging data}
%\title[MVPA-Light]{MVPA-Light: a classification suite for neuroimaging data}
\title[MVPA-Light]{MVPA-Light: a classification and regression toolbox for multi-dimensional data}

\author[\firstAuthorLast ]{\Authors} %This field will be automatically populated
\address{} %This field will be automatically populated
\correspondance{} %This field will be automatically populated

\extraAuth{}% If there are more than 1 corresponding author, comment this line and uncomment the next one.
%\extraAuth{corresponding Author2 \\ Laboratory X2, Institute X2, Department X2, Organization X2, Street X2, City X2 , State XX2 (only USA, Canada and Australia), Zip Code2, X2 Country X2, email2@uni2.edu}


\maketitle


\begin{abstract}

%%% Leave the Abstract empty if your article does not require one, please see the Summary Table for full details.
\section{}
MVPA-Light is a MATLAB toolbox for multivariate pattern analysis (MVPA). It provides native implementations of a range of classifiers (LDA, Logistic Regression, SVM, kernel FDA, Naive Bayes, ensemble methods) and regression models (ridge, kernel ridge), using modern optimization algorithms. High-level functions allow for the multivariate analysis of multi-dimensional data, including generalization (e.g. time x time) and searchlight analysis. The toolbox performs  cross-validation, hyperparameter tuning, and nested preprocessing. It computes various classification and regression metrics and establishes their statistical significance. It is modular, easily extendable, and is shipped with sample data and example scripts. Furthermore, it offers interfaces for LIBSVM and LIBLINEAR as well as an integration into the FieldTrip neuroimaging toolbox.



\tiny
 \keyFont{ \section{Keywords:} toolbox, software, machine learning, classification, decoding, regression, MVPA, SVM, Linear Discriminant Analysis, Logistic Regression, ridge regression, kernel methods, ensemble methods, regularization, cross-validation
 } %All article types: you may provide up to 8 keywords; at least 5 are mandatory.
\end{abstract}

%%% -------- INTRODUCTION --------
\section{Introduction}

During the early 2010s, multivariate pattern analysis (MVPA) became a mainstream statistical tool in EEG/MEG research \citep{Blankertz2011,Lemm2011,Mur2009,Grootswagers2017DecodingData,Pereira2009,Varoquaux2017}. It can be considered as a complement to the set of traditional statistical tools available to researchers (e.g. t-test, ANOVA). MVPA is particularly suited to large, multivariate datasets. Furthermore, since many classifiers make few or no assumptions about the data distribution, and statistical significance is often established using permutation tests, MVPA can be considered as a set of non-parametric statistical tools.
One of the primary applications of MVPA is \textit{classification}. In classification, an algorithm called \textit{classifier} takes a multivariate pattern of brain activity (referred to as \textit{features}) as input and maps it onto a categorical brain state or experimental condition (referred to as \textit{class label}). In other words, the task of a classifier is to predict class labels given features. The terms classification/classifier are also known as decoding/decoder in the cognitive neuroscience literature. It is worth noting that classification is often performed at the level of individual participants, taking trials as data samples. The classifier output then corresponds to predictions for single trials. Another important application of MVPA is \textit{regression}. It shares many characteristics with classification, but it maps features onto a continuous variable (e.g. reaction time) referred to a \textit{responses} rather than class labels. Consequently, predictions of the regression model are continuous  (e.g. predicted reaction time given a brain pattern).

A number of excellent MATLAB toolboxes for MVPA have been developed over the years, including the Amsterdam Decoding and Modelling Toolbox (ADAM) \citep{Fahrenfort2018FromADAM}, BCILAB \citep{Kothe2013BCILAB:Development}, Berlin BCI toolbox \citep{Blankertz2016TheControl}, CoSMoMVPA \citep{Oosterhof2016CoSMoMVPA:Octave}, Decision Decoding ToolBOX (DDTBOX) \citep{Bode2019ThePotentials}, Donders Machine Learning Toolbox (DMLT) (\texttt{github.com/distrep/DMLT}), Pattern Recognition for Neuroimaging Toolbox (PRoNTo) \citep{Schrouff2013PRoNTo:Toolbox}, and The Decoding Toolbox (TDT) \citep{Hebart2015TheData}. Beyond MATLAB, the probably most popular computer languages for machine learning are Python and R, with outstanding toolboxes such as Scikit Learn \citep{Pedregosa2011Scikit-learn:Python} for Python and Caret \citep{Kuhn2008BuildingPackage} and MLR \citep{Bischl2000Mlr:R} for R. A comprehensive comparison of MVPA-Light with all of these toolboxes is beyond the scope of this paper, but we believe that what sets MVPA-Light apart is the adherence to all of the following design principles: 

\begin{itemize}
\item \textit{self-contained}: unlike many toolboxes that provide wrappers for existing classifiers, the backbone of MVPA-Light is  native implementations of various classifiers, regression models, and their corresponding optimization algorithms (Trust-Region Newton, Dual Coordinate Descent). As a result, MVPA-Light works out-of-the-box, without the need for additional toolboxes or  code compilation.
\item \textit{transparent}: the toolbox has a shallow code base with well-documented functions. In many cases, the function call stack has a depth of two within the toolbox. For instance, a call to \texttt{mv\_classify} using an LDA classifier triggers calls to functions such as  \texttt{mv\_check\_inputs}, \texttt{train\_lda} and \texttt{test\_lda}. Although the train/test functions might call additional optimization functions, most of the work is done at these two shallowest levels. To preserve the shallowness, high-level functions replicate some code that might be shared otherwise. Furthermore, object orientation and encapsulation is avoided in favour of the more transparent MATLAB structs.
\item \textit{fast}: all models and high-level functions are written with speed as a prime concern. In some cases, the need for speed conflicts with the out-of-the-box requirement. For instance, Logistic Regression and SVM use iterative optimization algorithms written in MATLAB. However, these algorithms run faster using compiled code. To this end, an interface is provided for LIBSVM \citep{Chang2011LIBSVM:Machines} and LIBLINEAR \citep{Fan2008}, two excellent C implementations of Logistic Regression and SVM for users who do not shy away from compiling the code on their platform.
\item \textit{modular and pluggable}: it is possible, and intended, to harvest parts of the code such as the classifiers for other purposes. It is also easy to plug the toolbox into a larger EEG/MEG processing framework. An interface for FieldTrip \citep{Oostenveld2011} is described in the Methods section.
\item \textit{high-level interface}: common MVPA tasks such as searchlight analysis and time generalization including cross-validation can be performed with a few lines of MATLAB code. Many of the hyperparameters required by classifiers and regression models can be set to 'auto'; the parameters are then automatically selected by MVPA-Light, taking the burden of hyperparameter selection off the user. For an effortless start, a small EEG dataset and example scripts are shipped with the toolbox.
\end{itemize}

It is worth noting that MVPA-Light is a purely statistical toolbox. That is, it assumes that data has been preprocessed with a neuroimaging toolbox and comes in the shape of MATLAB arrays. Many neuroimaging toolboxes (e.g. FieldTrip, SPM, EEGLAB) store the imaging  data in such arrays, so that MVPA-Light can easily be used as a plugin tool. This comes with the perk that adaptation to different imaging modalities is straightforward.

\subsection{Getting started}

MVPA-Light is shipped with a set of example scripts (in the \ttt{/examples} subfolder) and an example EEG dataset. These scripts cover both the high-level functions in MVPA-Light and calling the train/test functions manually. The best starting point is to work through the example scripts and then adapt them to one's purpose. An up-to-date introduction to the toolbox with relevant hyperlinks is provided on the GitHub page (\ttt{github.com/treder/mvpa-light}).

The EEG data has been taken from the  BNCI-Horizon-2020 repository (http://bnci-horizon-2020.eu/database). It consists of three \ttt{mat} files corresponding to three subjects (subject codes \ttt{VPaak}, \ttt{VPaan}, and \ttt{VPgcc}) from the auditory oddball \red{paradigm} introduced in \cite{Treder2014}. Out of the experimental conditions, the "SynthPop" condition has been selected. Attended and unattended deviants are coded as class 1 and 2. The 64 EEG channels in the original dataset have been reduced to 32 channels.

To give a concrete code example, consider a [samples x channels] data matrix for one participant, where the samples correspond to trials and the channels serve as features. The matrix is denoted as \ttt{X}. Each trial corresponds to either condition 1 (e.g. oddball sound) or condition 2 (standard sound). This is encoded in a vector of class labels, denoted as \ttt{clabel}, that contains 1's and 2's. Then the following piece of code performs 10-fold cross-validation with 2 repetitions. LDA is used as classifier and area under the ROC curve (AUC) is calculated as a classification metric.

\begin{verbatim}
cfg = [];
cfg.model      = 'lda';
cfg.metric     = 'auc';
cfg.cv         = 'kfold';
cfg.k          = 10;
cfg.repeat     = 2;

perf = mv_classify(cfg, X, clabel);
\end{verbatim}

The output value \ttt{perf} contains the classifier performance measure, in this case a single AUC value averaged across test folds and repetitions. \ttt{mv\_classify} is part of the high-level interface that will be discussed next.

In the next section, the toolbox is described in more detail and example classification analyses are performed using the publicly available \cite{Wakeman2014OpenfMRI,Wakeman2015ADataset} MEEG dataset and the \cite{Haxby2001} fMRI dataset.

%%% ------------------------
%%% ------------------------
%%% -------- METHOD --------
%%% ------------------------
%%% ------------------------
\section{Materials and methods}

\subsection{Requirements}

A standard desktop computer is sufficient to run MVPA-Light. The RAM requirement is dictated by the memory footprint of the dataset. Since some functions operate on a copy of the data, it is recommended that the available RAM exceeds the size of the dataset by at least a factor of two (e.g. 4+ GB RAM for a 2 GB dataset). MVPA-Light is supported by MATLAB 2012a and more recent versions. The Statistics toolbox is required at some points in the toolbox (e.g. for calculating t-values). The cluster permutation test in \ttt{mv\_statistics} uses the Image Processing toolbox to extract the clusters.

%%% -----------------------------
%%% --- HIGH-LEVEL FUNCTIONS ----
%%% -----------------------------
\subsection{High-level interface}\label{sec:interface}

The structure of MVPA-Light is depicted in \prettyref{fig:structure}. The toolbox can be interacted with through high-level functions that cover common classification tasks. \ttt{mv\_classify} is a  general-purpose function that works on data of arbitrary dimension (e.g. time-frequency data). It performs any combination of cross-validation, searchlight analysis, generalization, and other tasks. Two more specialized functions are provided for convenience: \ttt{mv\_classify\_across\_time}  and \ttt{mv\_classify\_timextime}, assume that the data has a time dimension, i.e. it is a 3-D [samples $\times$ features $\times$ time points] array. \ttt{mv\_classify\_across\_time} performs classification for every time point, resulting in a vector of cross-validated metrics, the length of the vector being the number of time points. \ttt{mv\_classify\_timextime} implements time generalization \citep{King2014}, that is, classification for every combination of training and test time points, resulting in a 2-D matrix of cross-validated metrics. For regression tasks, the equivalent to \ttt{mv\_classify} is the function \ttt{mv\_regress}. It also works with data of arbitrary dimension and supports both searchlight and generalization.

All high-level functions take three input arguments. First, \ttt{cfg}, a configuration structure wherein parameters for the analysis can be set. Second, \ttt{X}, the data acting as input to the model. Third, \ttt{clabel} or \ttt{y}, a vector of class labels or responses. Some of the parameters in the \ttt{cfg} struct are common  to all high-level functions:

\begin{itemize}
    \item \ttt{cfg.model}: name of the classifier or regression model, e.g. \ttt{'lda'}.
    \item \ttt{cfg.hyperparameter}: a struct that specifies the hyperparameters for the model. For instance, \ttt{cfg.hyperparameter.lambda = 0.1} sets the magnitude of shrinkage regularization in LDA.
    \item \ttt{cfg.metric}: specifies the metric to be calculated from the model predictions. e.g. classification accuracy or mean-squared error for regression. Metrics are introduced in \prettyref{sec:metrics}.
    \item \ttt{cfg.preprocess}: a struct that specifies a nested preprocessing pipeline. The pipeline consists of preprocessing operations that are applied on train and test data separately. Preprocessing is discussed in \prettyref{sec:preprocessing}.
\end{itemize}

Additionally, the \ttt{cfg} struct includes the cross-validation settings explained in the next section.


\subsubsection{Cross-validation}

To obtain a realistic estimate of classification or regression performance and control for overfitting, a model should be tested on an independent dataset that has not been used for training. In most neuroimaging experiments, there is only one dataset with a restricted number of trials. K-fold cross-validation makes efficient use of such data by splitting it into k different folds. In every iteration, one of the k folds is held out and used as test set, whereas all other folds are used for training. This is repeated until every fold served as test set once. Since cross-validation itself is stochastic due to the random assignment of samples to folds, it can be useful to repeat the cross-validation several times and average the results. See \cite{Lemm2011,Varoquaux2017} for a discussion of cross-validation and potential pitfalls. Cross-validation is implemented in all high-level functions. It is controlled by the following parameters:

\begin{itemize}
    \item \ttt{cfg.cv}: cross-validation type, either \ttt{'kfold'}, \ttt{'leaveout'}, \ttt{'predefined'}, \ttt{'holdout'} or \ttt{'none'}.
    \item \ttt{cfg.k}: number of folds in k-fold cross-validation.
    \item \ttt{cfg.repeat}: number of times the cross-validation is repeated with new randomly assigned folds.
    \item \ttt{cfg.p}: if \ttt{cfg.cv = 'holdout'}, \ttt{p} is the fraction of test samples.
    \item \red{\ttt{cfg.fold}: if \ttt{cfg.cv = 'predefined'}, fold is a vector that  specifies which fold a sample belongs to.}
    \item \ttt{cfg.stratify}: if 1, for classification, the class proportions are approximately preserved in each test fold.
\end{itemize}

See the function \ttt{mv\_get\_crossvalidation\_folds} for more details.

\subsubsection{Hyperparameter tuning}

Hyperparameters are parameters of a classifier or regression model that need to be specified by the user, such as the type and amount of regularization applied, the type of kernel, and the kernel width $\gamma$ for Gaussian kernels. From the user's perspective, hyperparameters can be nuisance parameters: it is sometimes not clear a priori how to set them, but their exact value can have a substantial effect on the result.

MVPA-Light tries to automate hyperparameter selection as much as possible. This is done using either reasonable default values, hyperparameter estimators (\cite{Ledoit2003HoneyMatrix} for LDA) or  hyperparameter-free regularizers (log-F(1,1) for Logistic Regression). If this is not possible, automated grid search using nested cross-validation can be used for testing out different hyperparameter combinations essentially by brute force. For better performance, bespoke hyperparameter tuning functions are implemented for some classifiers. Otherwise, the generic tuning function \ttt{mv\_tune\_hyperparameters} is used.

%%% ------- PREPROCESSING -------
\subsubsection{Preprocessing}\label{sec:preprocessing}

Preprocessing refers to operations applied to the data prior to training the classifier. To not bias the result, some preprocessing operations (such as Common Spatial Patterns) should be performed in a 'nested' fashion. That is, they are performed on the training data first and subsequently applied to the test data using parameters estimated from the training data \citep{Lemm2011,Varoquaux2017}. 
Currently implemented functions include PCA, sample averaging \citep{Cichy2017MultivariateSpace}, kernel averaging \citep{Treder2018}, and under-/oversampling for unbalanced data. Preprocessing pipelines are defined by adding the \ttt{cfg.preprocess} parameter. For instance,

\begin{verbatim}
cfg.preprocess = {'undersample', 'zscore', 'average_kernel'}
\end{verbatim}

adds a preprocessing pipeline that perfoms undersampling of the data followed by z-scoring and kernel averaging.

%%% ------- SEARCHLIGHT -------
\subsubsection{\red{Searchlight analysis}}\label{sec:searchlight}

\red{In neuroimaging analysis, a question such as "Does brain activity differentiate between faces and houses?" is usually less interesting than the question "\textit{Which brain regions} differentiate between faces and houses?". In other words, the goal of MVPA is to establish the presence of a significant difference \textit{and} localize it in space and/or time.}

\red{Classical univariate approaches such as t-tests offer perfect localizability since tests are performed at each voxel/sensor/time point separately. However, they lack statistical power due to the distributed nature of neural representations \citep{Haxby2001} and suffer from the multiple comparisons problem. Multivariate approaches  offer high predictive power by combining  sources of information, but they often lack the ability to identify discriminative features.}

\red{Searchlight analysis intends to marry localizability with predictive power. It is a well-established technique in the fMRI literature, where a searchlight is defined as a ball of e.g. 1 cm radius, centered on a voxel in the brain \citep{Kriegeskorte2006Information-basedMapping}. All voxels within the radius serve as features for a classification or regression analysis. The result of the analysis is assigned to the central voxel. If the analysis is repeated for all voxel positions, a 3D map of e.g. classification accuracies results that can be overlayed on a brain image. Brain regions that have discriminative power then correspond to peaks in the map. The same idea can be applied to other dimensions such as time points and  frequencies.}

\red{In MVPA-Light, \ttt{mv\_classify\_across\_time} performs a searchlight across the time axis. More bespoke searchlight analyses can be conducted using \ttt{mv\_classify} and \ttt{mv\_regress} by setting the parameter \ttt{cfg.neighbours}.}

%%% -----------------------------
%%% -------- CLASSIFIERS --------
%%% -----------------------------
\subsection{Classifiers}\label{sec:classifiers}

\red{\prettyref{fig:classifiers} provides a pictorial description of the classifiers. They are implemented using pairs of train/test functions. In the high-level interface, a classifier and its hyperparameters can be specified using \ttt{cfg.model} and \ttt{cfg.hyperparameter}. For instance, }

\begin{verbatim}
cfg.model = 'lda';
cfg.hyperparameter.lambda = 0.1;
\end{verbatim}

\red{specifies an LDA classifier and sets the hyperparameter $\lambda = 0.1$. The \ttt{cfg} struct can then be used in a high-level function call, e.g. \ttt{acc = mv\_classify(cfg, X, clabel)}.
Alternatively, as a low-level interface, the train/test functions can be called directly. For instance, an LDA classifier can be trained directly using}

\begin{verbatim}
model = train_lda(param, X, clabel)
\end{verbatim}

where \ttt{X} is the training data and \ttt{clabel} are the corresponding class labels. \ttt{param} is a MATLAB struct that contains hyperparameters (same as \ttt{cfg.hyperparameter}). The hyperparameters for LDA can be accessed by typing \ttt{help(train\_lda)} in MATLAB. 
The output \ttt{model} is a struct that contains the classifier's parameters after training. The classifier can be applied to test data, denoted as \ttt{Xtest}, by calling

\begin{verbatim}
[clabel, dval, prob] = test_lda(model, Xtest)
\end{verbatim}

The first output argument \ttt{clabel} is the \textit{predicted} class labels. They can be compared against the true class labels to calculate a classification performance metric. \ttt{test\_lda} provides two additional outputs, but not all classifiers have this capability. \ttt{dval} is the \textit{decision value}, a dimensionless quantity that measures the \textit{distance to the hyperplane}. \ttt{prob} contains the probability for a given sample to belong to class 1. 

\red{To introduce some mathematical notation needed in the following, data is denoted as a matrix $\X\in\R^{n \times p}$ of $n$ samples and $p$ predictors/features. The i-th row of $\X$ is denoted as the column vector $\x_i\in\R^p$. Class labels are stored in a vector $\mathbf{y}\in\R^n$ with $y_i$ referring to the i-th class label. When the index is not relevant, the feature vector and class label are simply referred to as $\x$ and $y$. Before delving into the description of the classifiers, two conceptual perspectives are introduced that highlight some of the similarities and differences between the models.}

\subsubsection{\red{Perspective 1: Linear classifiers}}

\red{For two classes, linear classifiers such as LDA, Logistic Regression, and linear SVM act on the data in a unified way. The decision value for a test sample $\x$ is given by }

\begin{equation}
\label{eq:linear_dval}
\text{dval} = \w^\top\x + b
\end{equation}

\red{where $\w$ is the weight vector or normal to the hyperplane specifying the linear combination of features, and $b$ is the threshold/bias term.  A sample is then  assigned to the first class if \ttt{dval}$>0$ and to the second class if \ttt{dval}$<0$. If we encode class 1 as +1 and class 2 as -1, this can be expressed succinctly as }

\begin{equation*}
\text{predicted clabel} =\text{sign} \left(\w^\top\x + b\,\right)
\end{equation*}

\red{where $\text{sign}:\R\rightarrow\{-1,+1\}$ is the sign function. Linear classifiers differ only in the way that $\w$ and $b$ are derived. }

\subsubsection{\red{Perspective 2:  probabilistic classifiers}}

\red{Another useful perspective for elucidating the relationship between  classifiers is given by the Bayesian framework \citep{Bishop2007}. Probabilistic classifiers such as LDA, Naive Bayes, and Logistic Regression are able to directly model class probabilities for individual samples. Let us denote the (posterior) probability that a test sample $\x$ belongs to class $i$ as $P(y = i\,|\,\x)$. One possible approach for calculating this quantity is Bayes' theorem:}

\begin{equation}
\label{eq:bayes}
P(y = i\,|\,\x) = \frac{P(\x\,|\,y = i)\  P(y = i)}{P(\x)}
\end{equation}

\red{Here, $P(\x\,|\,y = i)$ is the likelihood function which quantifies the relative probability of observing  and $P(y = i)$ is the prior probability for a sample to belong to class $i$. The denominator, called evidence, can be calculated by marginalizing across the classes: $P(\x) = \sum_i P(\x\,|\,y = i)\  P(y = i)$.} 

%%% -------- LDA --------
\subsubsection{Linear Discriminant Analysis (LDA)}\label{sec:lda}

\red{If the classes follow a multivariate Gaussian distribution with a common covariance matrix for all classes, LDA yields the theoretically optimal classifier \citep{Duda1998}. In the context of EEG/MEG analysis, LDA is discussed in detail in \cite{Blankertz2011}. The likelihood function takes the form}

\begin{equation}
\label{eq:lda_likelihood}
P(\x\,|\,y = i) \sim \mathcal{N}(\m_i, \mathbf{\Sigma})
\end{equation}

\red{with a class-specific mean $\m_i$ and common covariance matrix $\mathbf{\Sigma}$ which have to be  estimated from the training data. \prettyref{eq:bayes} can be evaluated directly to calculate class probabilities. A prediction can be done by selecting the most likely class out of all candidate classes, }

\begin{equation*}
\text{predicted class} = \underset{i}{\text{arg max}}\ P(y = i\,|\,\x)
\end{equation*}

\red{which is known as the maximum a posteriori (MAP) rule. LDA is closely related to other statistical models. For two classes, LDA is equivalent to linear regression using the class labels as targets. It is also equivalent to LCMV beamforming when applied to ERP data \citep{Treder2016}. The latter equivalence relationship also applies to other methods based on generalized eigenvalue decomposition of covariance matrices \citep{DeCheveigne2014JointAnalysis}.}

\red{In MVPA-Light, multi-class LDA is implemented as the classifier  \texttt{'multiclass\_lda'}. For two classes, a more efficient implementation is available denoted as  \ttt{'lda'}. In practice, the covariance matrix is often ill-conditioned and needs to be regularized \citep{Blankertz2011}. The hyperparameter \ttt{lambda} controls the amount of regularization. In shrinkage regularization,  \ttt{lambda} $\in [0,1]$ blends between the empirical covariance matrix (\ttt{lambda}=0) and a scaled identity matrix (\ttt{lambda}=1). By default, \ttt{lambda} is estimated automatically using the Ledoit-Wolf formula \citep{Ledoit2003HoneyMatrix,Blankertz2011}. \prettyref{sec:lda_implementation} of the appendix discusses the implementation of LDA in detail.} 


%%% -------- NAIVE BAYES --------
\subsubsection{Naive Bayes}

\red{In Naive Bayes, the features are assumed to be conditionally independent of each other given the class label \citep{Bishop2007}. While this is indeed naive and often wrong, Naive Bayes has nevertheless been remarkably successful in classification problems. The independence assumption leads to a straightforward formula for the likelihood function since only univariate densities need to be estimated. Let $\xf{j}$ be the $j$-th feature and $\x = [\xf{1}, \xf{2}, ..., \xf{p}]^\top$ be a feature vector then the likelihood function is given by }

\begin{equation*}
P(\x\,|\,y = i) = \prod_{j=1}^p P(\xf{j}\,|\,y = i)\, P(y = i)
\end{equation*}

\red{Like in LDA, the predicted class can be obtained using the MAP rule. In MVPA-Light, Naive Bayes is implemented as \texttt{'naive\_bayes'}. Additionally, MVPA-Light assumes that these densities are univariate Gaussian, i.e. $P(\xf{j}\,|\,y = i) \sim \mathcal{N}(m_{ij}, \sigma^2_{ij})$. For Gaussian densities, the independence assumption is equivalent to assuming that the covariance matrix is diagonal. As indicated in \prettyref{fig:classifiers}, there is a close relationship between LDA and Gaussian Naive Bayes: LDA allows for a dense covariance matrix, but it requires that it is the same for all classes. In contrast, Naive Bayes allows each class to have a different covariance matrix, but it requires each matrix to be diagonal. Additional details on the implementation are given in \prettyref{sec:naive_bayes_implementation}.} 

%%% -------- LOGISTIC REGRESSION --------
\subsubsection{Logistic Regression}

\red{In Logistic Regression for two classes, the posterior probability is modeled directly by fitting a logistic function to the data \citep{Hastie2009}. If the two classes are coded as +1 and -1, it is given by}

\begin{equation}
\label{eq:logreg_probability}
P(y = \pm 1\,|\,\x) = \frac{1}{1 + \text{exp}(-y\,(\w^\top\x + b))}
\end{equation}

\red{The weights $\w$ are found by minimizing the logistic loss function}

\begin{equation}
\label{eq:logreg_loss_function}
\L_\text{LR}(\w) = \sum_{i=1}^n \log(1 + e^{-y_i(\w^\top\x_i + b)})
\end{equation}

\red{In MVPA-Light, Logistic Regression  is implemented as \texttt{'logreg'}. By default, log-F(1,1) regularization (\ttt{reg = 'logf'}) is used by imposing Jeffrey's prior on the weights \citep{Firth1993BiasEstimates,Rahman2017PerformanceData.,King2001}. Alternatively, L2-regularization can be used to impose a Gaussian prior on the weights (\ttt{reg = 'l2'}). In this case, an additional hyperparameter \ttt{lambda} $\in [0,\infty)$ that controls the amount of regularization needs to be specified by the user. It can be set to a fixed value by the user. Alternatively, a range of candidates can be specified (e.g. \ttt{lambda = [0.001, 0.01, 0.1, 1]}). A nested cross-validation is then performed to select the optimal value. Additional details on the implementation are given in \prettyref{sec:logreg_implementation}. An alternative implementation using LIBLINEAR is also available, see \prettyref{sec:libsvm}.}

%%% -------- SVM --------
\subsubsection{Linear Support Vector Machine (SVM)}

\red{A SVM has no underlying probabilistic model. Instead, it is based on the idea of maximizing the margin \citep{Scholkopf2001LearningBeyond,Hearst1998SupportMachines}. For linearly separable data, the margin is the distance from the hyperplane to the closest data point (dotted line in \prettyref{fig:classifiers}). This distance is given by $1/||\w||$. Minimizing $||\w||$ is then equal to maximizing the margin. At the same time, one needs to make sure that the training samples are correctly classified at a distance from the hyperplane. This can be achieved by requiring 
$\w^\top\x_i + b\ge 1$ for class 1 and $\w^\top\x_i + b\le -1$ for class 2. Encoding the classes as +1 and -1, both terms can be combined into $y_i\, (\w^\top\x_i + b) \ge 1$. Unfortunately, this constraint cannot be satisfied for every training sample $i\in\{1,...,n\}$ if the data cannot be perfectly separated. To this end, positive slack variables $\xi_i$ are introduced that allow for misclassifications. Now the goal becomes to maximize the margin while simultaneously minimizing the amount of misclassifications given by $\sum_i \xi_i$. Put together, this leads to the following optimization problem: }

\begin{align}
\begin{split}
\label{eq:svm_primal}
\underset{w}{\text{arg min}} &\ \frac{1}{2}||\w||^2 + c\sum_i \xi_i\\
\text{subject to } &\ \forall i: y_i (\w^\top\x + b) \ge 1 - \xi_i\\ 
&\ \forall i: \xi_i \ge 0\\ 
\end{split}
\end{align}

\red{The resultant classifier, called two-class L1-Support Vector Machine (SVM) is implemented as \ttt{'svm'}. The hyperparameter \ttt{c} controls the amount of regularization and needs to be set by the user. Despite the lack of a probabilistic model, a Platt approximation using an external function (\ttt{http://www.work.caltech.edu/~htlin/program/libsvm/}) can be used to estimate class probabilities if required. Additional details on the implementation are given in \prettyref{sec:svm_implementation}. Alternative implementations using LIBSVM and LIBLINEAR are also available, see \prettyref{sec:libsvm}.}

%%% -------- KERNEL METHODS --------
\subsubsection{Kernel classifiers}\label{sec:kernel-methods}

\red{In kernel methods such as SVM and kernel FDA, a sample is implicitly mapped from the input space $\mathcal{X}$ into a high-dimensional feature space $\mathcal{F}$ using a map $\phi:\mathcal{X}\rightarrow\mathcal{F}$. As illustrated in \prettyref{fig:classifiers}, such a map can translate a non-linear classification problem into linear problem in feature space \citep{Scholkopf2001LearningBeyond}. For two classes, decision values are given by }

\begin{equation}
\label{eq:kernel_dval}
\text{dval} = \w^\top\phi(\x) + b
\end{equation}

\red{If we compare this formula to \prettyref{eq:linear_dval}, it becomes evident that kernel classifiers are linear classifiers acting on non-linear transformations of the features. Often, it is  infeasible to explicitly apply the map due to the high dimensionality of $\mathcal{F}$.  However, for methods such as SVM and LDA, a very efficient workaround is available. They can be rewritten into a form wherein only the inner products between pairs of samples are needed, i.e. $\phi(\x)\cdot\phi(\x')$ for samples $\x$ and $\x'$. Now, if $\phi$ maps to a Reproducing Kernel Hilbert Space (RKHS), these inner products can be efficiently calculated via a kernel function $k$ that operates in input space, resulting in the identity $k(\x,\x') = \phi(\x)\cdot\phi(\x')$. This is known as the \textit{kernel trick}.}

\red{To give a simple example, consider two samples with  two-dimensional features, $\x = [x_1, x_2]$ and $\x' = [x_1', x_2']$. The homogeneous polynomial kernel of degree 2 has the kernel function $k(\x,\x') = (\sum_{i=1}^2 \x_i\x_i')^2$ and the corresponding feature map $\phi:\R^2\rightarrow\R^3$ with $\phi(\x)= [x_1^2, \sqrt{2}x_1 x_2, x_2^2]$. It is now easily verified that $k(\x,\x') = \phi(\x)\cdot\phi(\x')$. For LDA, a kernelized version called Kernel Fisher Discriminant Analysis (KFDA) has been developed by \citep{Mika1999FisherKernels}. It is available as \ttt{'kernel\_fda'}. Like in LDA, the model can be regularized using ridge regularization (\ttt{reg = 'ridge'}) or shrinkage regularization (\ttt{reg = 'shrink'}). The regularization hyperparameter \ttt{lambda} needs to be set by the user. For shrinkage, in many cases a small value (e.g. \ttt{lambda = 0.01}) is adequate. For both SVM and KFDA, the kernel can be chosen by setting the \ttt{kernel} parameter. Further information on the kernels is provided in the train functions. }

%%% -------- ENSEMBLES --------
\subsubsection{Ensemble methods}

The \ttt{'ensemble'} classifier implements a meta-classifier that trains dozens
or even hundreds of classifiers such as LDA. The rationale behind training a large number of classifiers is that if each classifier manages to learn slightly different aspects about the data, their joint performance can be better than the performance of any individual classifier.
In ensemble methods, these individual classifiers are called \textit{learners}. In MVPA-Light, learners are set using the \ttt{learner} hyperparameter. For instance, setting \ttt{learner = 'lda'} trains a number of LDA classifiers. To force the learners to focus on different aspects of the data, every learner is presented just a subset of the training data. \ttt{nsamples} controls the number of training samples that is randomly subselected for a given learner, whereas \ttt{nfeatures} controls the number of features.
The final classifier output is determined via a voting strategy. If \ttt{strategy = 'vote'}, then the class label produced by each individual learner serves as a vote. The class that receives the maximum number of votes is then selected. If \ttt{strategy = 'dval'}  then the raw decision values are averaged and the final decision is taken based on whether the average is positive or negative. The latter only works with classifiers that produce decision values.


%%% ---------------------------------
%%% ----   CLASSIFIER OUTPUT     ----
%%% ---------------------------------
\subsection{Classifier output type}\label{sec:output}

For every test sample, a classifier produces raw output. This output takes either a discrete form, as a class label (e.g. the integer 1 for class 1), or a continuous one. If it is continuous, it comes either as a \textit{decision value} or as a \textit{probability}. A decision value is an unbounded number that can be positive or negative. Its absolute value corresponds to the distance to the hyperplane. For two classes, the probability is a number between 0 and 1 representing the probability that a sample belongs to class 1. 
In the high-level interface, the classifier output can be specified explicitly by setting \ttt{cfg.output\_type} to \ttt{'clabel'}, \ttt{'dval'}, or \ttt{'prob'}. In most cases, however, it suffices to let MVPA-Light infer the output type.

%%% -----------------------------
%%% -------- REGRESSION  --------
%%% -----------------------------
\subsection{Regression models}\label{sec:regression}

Like classifiers, regression models are implemented using pairs of train/test functions. In the high-level function \ttt{mv\_regress}, a regression model is specified using the \ttt{cfg.model} parameter. Low-level access is possible by directly calling the train/test functions. For instance, \ttt{model = train\_ridge(param, X, y)} trains a ridge regression model. \ttt{X} is the training data and \ttt{y} are the corresponding responses. \ttt{param} is a MATLAB struct that contains hyperparameters. The output \ttt{model} is a struct that contains the model parameters after training. The model can be applied to test data by calling \ttt{yhat = test\_ridge(model, Xtest)} where \ttt{Xtest} is test data. The output of the test function is the model predictions. In the following section, the individual regression models are introduced. It is assumed that the data is contained in matrix $\X\in\R^{n \times p}$ of $n$ samples and $p$ predictors. The i-th row of this matrix is denoted as the column vector $\x_i\in\R^p$. Responses are stored in a vector $\y\in\R^n$ with $\y_i$ referring to the i-th response.

%%% -------- RIDGE --------
\subsubsection{Ridge regression}

Ridge regression is a regularized version of ordinary least-squares (OLS) regression that is useful for analyzing data that suffers from multicollinearity. The model is regularized using a penalty that shrinks the weights towards zero. For a given regularization parameter $\lambda\in[0,\infty)$, the primal and dual ridge solutions are given by


\begin{align}
\begin{split}
\label{eq:ridge}
\w =\ & (\X^\top\X + \lambda\I_p)^{-1}\ \X^\top\y \quad\text{(primal solution)}\\
 =\ & \X^\top (\X\X^\top + \lambda\I_n)^{-1}\ \y \quad\text{(dual solution)}
\end{split}
\end{align}

where $\I_p\in\R^{p\times p}$ and $\I_p\in\R^{n\times n}$ are identity matrices. For $\lambda=0$ ridge regression reduces to ordinary least-squares regression. By default (\ttt{form = 'auto'}), MVPA-Light dynamically switches between the primal and the dual form depending on whether $n$ is larger or smaller than $p$.

%%% -------- KERNEL RIDGE --------
\subsubsection{Kernel ridge regression}

A kernelized version can be developed from the dual solution to ridge regression in a straightforward manner. Let $\phi:\mathcal{X}\mapsto\mathcal{F}$ be a map from input space to a Reproducing Kernel Hilbert Space, and $\Phi(\X) = [\phi(\x_1),\phi(\x_2),...,\phi(\x_n)]^\top$. Let $\K = \phi(\X)\phi(\X)^\top$ be the corresponding kernel matrix. Then the vector of dual weights is given by  

\begin{align}
\begin{split}
\label{eq:kernel_ridge}
\al = (\K + \lambda\I_n)^{-1}\ \y.
\end{split}
\end{align}

Predictions for a new vector $\x$ are obtained by $f(\x) = \sum_{i=1}^n k(\x, \x_i)\al_i$, where $k$ is the kernel function.


%%% ---------------------------------
%%% ----   PERFORMANCE METRICS   ----
%%% ---------------------------------
\subsection{Performance metrics}\label{sec:metrics}

In most cases, the quantity of interest is not the raw model output but rather a metric that summarizes the performance of the model on test data. The desired metric can be specified by e.g. setting \ttt{cfg.metric = 'accuracy'} in any high-level function. Multiple metrics can be requested by providing a cell array, e.g. \ttt{cfg.metric = \{'accuracy', 'auc'\}}. \prettyref{tab:metrics} lists the metrics implemented in MVPA-Light. For a thorough discussion of classification metrics, refer to  \cite{Sokolova2009ATasks}.

\begin{table}[]
\begin{tabularx}{\textwidth}{lllX}
Task & Metric &  Range & Description   \\\hline
Classification & accuracy  &  [0,1] &  Fraction correctly predicted class labels.\\
 & AUC & [0,1] & For two classes only. An alternative to classification accuracy that is more robust to imbalanced classes. Requires continuous classifier output (decision values or probabilities). 0.5 means chance-level performance and 1 means perfect separation of the classes.\\
 & confusion & [0,1] &  Confusion matrix. Rows corresponds to true class, columns  to predicted class. The $(i,j)$-th element gives the proportion of samples of class i that have been classified as class j.\\
 &dval & ($-\infty,+\infty$) & For two classes only. Average decision value, for each class separately.\\
 &F1 & [0,1] & Combines precision and recall into a single score using the harmonic average 2*PR*R / (PR+R).\\
&kappa & [-1, 1] & Cohen's kappa, a measure of inter-rater reliability.\\
&precision (PR) & [0,1] & TP / (TP + FP). Fraction of samples labeled as positive that actually belong to the positive class. For multi-class, it is calculated per class from the confusion matrix.\\
&recall (R) & [0,1] & TP / (TP + FN). Fraction of positive samples that have been detected. For multi-class, it is calculated per class from the confusion matrix.\\
&tval & ($-\infty,+\infty$) & For two classes only. T-test statistic for the unequal sample size, equal variance case, based on decision values.\\
&none & ($-\infty,+\infty$) & Returns a cell array with the raw classifier outputs for all test sets.\\\hline
Regression & MAE  & [$0,\infty$) &  Mean absolute error: $1/n \sum_{i=1}^n|y_i - \hat{y}_i|$.\\
& MSE  & [$0,\infty$) &  Mean squared error: $1/n \sum_{i=1}^n|y_i - \hat{y}_i|^2$.\\
& $R^2$  & ($-\infty,1$] &  Fraction variance explained.\\
\hline
\end{tabularx}
\caption{Metrics in MVPA-Light. Abbreviations for classification: TP = true positives, FP = false positives, FN = false negatives. Regression: $y$ = responses, $\hat{y}$ = model predictions.}
\label{tab:metrics}
\end{table}

If cross-validation is used then the metric is initially calculated for each test set in each repetition separately. It is then averaged across test sets and repetitions. Since the number of samples in a test set can vary across different folds, a proportionally weighted average is used whereby larger test sets get a larger weight.

%%% ---------------------------------
%%% ----   STATISTICS   ----
%%% ---------------------------------
\subsection{Statistical analysis}\label{sec:statistics}

In many neuroimaging experiments, establishing the statistical significance of a classification or regression metric is more important than maximizing the metric per se. \red{Neuroimaging data is typically hierarchical: a study comprises many subjects, and each subject comprises many trials. To perform group analysis, a common approach is then to start with a Level 1 (single-subject) analysis, e.g. calculating the means for each subject in two different experimental conditions. At this stage, the samples consist of single trials for a particular subject. The means are then taken on to Level 2 (group level). At this stage, each subject constitutes one sample  \citep{Mumford2007ModelingData}}. 
The function \ttt{mv\_statistics} implements both Level 1 (single-subject) and Level 2 (group level) statistical analysis. For Level 1 analysis, the following tests are available:

\begin{itemize}
    \item Binomial test: uses a binomial distribution to calculate the p-value under the null hypothesis that classification accuracy is at chance. Requires classification accuracy as metric.
    \item Permutation test: non-parametric significance test. Creates a null distribution by shuffling the class labels or responses and repeating the multivariate analysis e.g. 1000 times. 
    \item Cluster permutation test: an elegant solution to the multiple comparisons problems arising when MVPA is performed along multiple dimensions (e.g. for each time-frequency point). Uses the cluster statistic introduced in 
    \cite{Maris2007}.
\end{itemize}

For Level 2 analysis, a permutation test (with and without cluster correction) is available for within-subject and between-subjects designs.

\subsection{Custom classifiers and regression models}

MVPA-Light can easily be extended with custom models. To this end, the appropriate train and test functions need to be implemented. Additionally, default hyperparamaters need to be added to the function \ttt{mv\_get\_hyperparameter}. In the appendix, it is shown how to implement a prototype classifier that assigns a sample to the closest class centroid.

\subsection{LIBSVM and LIBLINEAR}\label{sec:libsvm}

LIBSVM \citep{Chang2011LIBSVM:Machines} and LIBLINEAR \citep{Fan2008} are two high-performance libraries for SVM, Support Vector Regression (SVR), and Logistic Regression. In order to use the libraries with MVPA-Light, the user needs to follow the installation instructions on the respective websites. In particular, the C-code needs to be compiled and added to the MATLAB path. In MVPA-Light, the models are denoted as \ttt{'libsvm'} and \ttt{'liblinear'}.

\subsection{FieldTrip integration}

The FieldTrip \citep{Oostenveld2011} function \ttt{ft\_statistics\_mvpa} provides a direct interface between FieldTrip and MVPA-Light. In brief, the function calls MVPA-Light functions to carry out multivariate analysis, and then stores the results back into FieldTrip structs. To use MVPA-Light from high-level FieldTrip functions such as \ttt{ft\_timelockstatistics}, one has to set the parameter \ttt{cfg.method = 'mvpa'}. The interface is introduced in detail in a tutorial on the FieldTrip website \footnote{\ttt{http://www.fieldtriptoolbox.org/tutorial/mvpa\_light/}}.

%%% -------------------------
%%% ----  DEVELOPMENT    ----
%%% -------------------------
\subsection{Development}\label{sec:development}

Apart from the \ttt{main} branch intended for general use, the GitHub repository features a \ttt{devel} branch wherein new code is developed before committing into the main branch. Additionally, the branch features code that helps in maintaining the integrity of the toolbox. The \ttt{unittests/} subfolder features a unit testing framework with specific unit tests for all models, optimization algorithms, high-level functions and some of the important utility functions. The unit tests make use of both the example EEG data, random noise, and simulated data. Unit testing can be triggered by executing the \ttt{run\_all\_unittests} function.

\subsection{Analysis of a MEEG dataset}

To illustrate MVPA-Light on a real dataset, a multivariate analysis was conducted on a multi-subject, multi-modal face processing dataset \citep{Wakeman2015ADataset,Wakeman2014OpenfMRI}. The dataset contains 16 participants with EEG and MEG simultaneously recorded. The MEEG data was preprocessed using FieldTrip. It was low-pass filtered with a cut-off of 100 Hz and high-pass filtered using a FIR one-pass zero-phase filter with a cut-off of 0.1 Hz. A bandstop filter was applied at 50 Hz to suppress line noise. Subsequently, data was downsampled to 220 Hz and for each subject, the 6 separate runs were combined into a single dataset, yielding 880--889 trials per subject with roughly equal proportions for the three classes. All trials displaying famous faces were coded as class 1, familiar faces as class 2, and scrambled faces as class 3. MVPA was performed to investigate the following questions:

\begin{enumerate}
    \item ERP classification: \cite{Wakeman2015ADataset} found two prominent event-related components, a N170 and a sustained component roughly starting at 400 ms post-stimulus. Cross-validation with a multi-class classifier was used to investigate whether these components discriminate between the three classes and, if so, between which of them.
    \item Time classification: Is there more discriminative information in MEG than in EEG? To answer this, classification across time was performed for three different channel sets, namely EEG only, MEG only, and EEG+MEG combined. 
    \item Time-frequency classification: Is the discriminative information for famous vs scrambled faces confined to specific oscillatory frequencies and times? To answer this, time-frequency spectra were calculated for single trials and classification was performed at each time-frequency bin separately. 
    \item Generalization: Are representations shared across time \citep{King2014} or frequency? To answer this, time generalization (time x time classification) was applied to the ERF data, and frequency generalization (frequency x frequency classification) was applied to the time-frequency data.
\end{enumerate}

MVPA was performed at the sensor level using a LDA classifier. All analyses were cross-validated using 5-fold or 10-fold cross-validation. Only the MEG channels were used as features except for analysis 2, where different sets of channels were compared. To assess statistical significance, the following analyses were carried out:

\begin{itemize}
    \item Level 1 statistics. The goal was to establish the statistical significance for the time generalization contrasting famous vs scrambled faces using the EEG channels only (first subject only). For illustrative purposes, the three statistical tests contained in MVPA-Light were compared: binomial, permutation, and cluster permutation tests. Permutation tests were based on 500 random permutations of the class labels. The cluster permutation test was corrected for multiple comparisons by using a cluster statistic, the other tests were uncorrected. For the cluster statistic, a critical value of 0.6 was chosen for  classification accuracy.
    \item Level 2 statistics (across subjects). The AUC values obtained in the time-frequency classification analyses were statistically compared to a null value of 0.5 using cluster permutation tests based on a within-subject design.
    
\end{itemize}

\subsection{Analysis of a fMRI dataset}
To illustrate the application of MVPA-Light to fMRI data, another analysis was conducted using the \cite{Haxby2001} dataset. It comprises the BOLD responses of 6 participants viewing images in eight different sessions. Images were taken from 8 different living and non-living categories. The dataset was downloaded from \ttt{http://www.pymvpa.org/datadb/haxby2001.html}. A zero-phase Butterworth high-pass filter with a cut-off frequency of 0.01 Hz was applied in order to remove slow drifts. No other preprocessing was performed. The following questions were addressed:

\begin{enumerate}
    \item Confusion matrix: Which image categories lead to similar brain activation patterns?
    \item Time classification: How does classification performance evolve across time following stimulus onset?
    \item Searchlight analysis: Which of the brain regions contain discriminative information that discerns between faces and houses?
\end{enumerate}

 Leave-one-session-out cross-validation was used to calculate classification performance. Multi-class LDA with 8 classes served as a classifier. For the searchlight analysis, binary LDA contrasting faces vs houses was used with AUC serving as metric. The searchlight consisted of a 3x3x3 cube of voxels that was centered on each target voxel. A level 2 cluster permutation test was computed on the AUC values against the null hypothesis that AUC equals 0.5. 

\subsection{Benchmarking}

%In multi-dimensional neuroimaging data, a model needs to be trained on a single subject's data potentially thousands of times. This is exacerbated in permutation testing, when this process needs to be repeated hundreds or thousands of times. 
Multivariate analyses can involve hundreds or even thousands of train/test iterations. Therefore,  training time (the amount of time required to fit a single model on data) is a relevant quantity when evaluating different model implementations. To benchmark MVPA-Light's models, their training time was compared to models in the MATLAB Statistics Toolbox as well as models in Python (Scikit Learn package) and R (different packages). The comparison to other MVPA toolboxes is of less relevance since they often rely on external packages such as LIBSVM and LIBLINEAR which are also available in MVPA-Light (this applies to e.g. DDTBOX, PRoNTo, TDT). The following three datasets were considered:

%A key variable to computational efficiency is training speed, that is, the amount of time needed to optimize the model parameters on training data. 

\begin{itemize}
    \item MEG single-subjects. The \cite{Wakeman2015ADataset} dataset was used with the famous vs scrambled faces conditions, epoched in the range [-0.2, 1] s. Data dimensions were 585--592 trials, 306 channels, and 265 time points. MVPA was performed for every subject and every time point separately, using channels as features.
    \item MEG super-subject. Trials of all subjects in the MEG single-subjects data were concatenated to form a single 'super-subject' dataset comprising 9,421 trials, 306 channels, and 265 time points. MVPA was performed for every time point, using channels as features.
    \item fMRI. For each subject in the \cite{Haxby2001} data, all voxels with a non-zero signal were concatenated to a single feature vector. The classes face vs house were considered, yielding a data matrix of 216 samples (198 samples for subject 5) and between 163,665 and 163,839 voxels per subject. MVPA was performed for every subject separately, using voxels as features.
\end{itemize}

The MEG single-subjects dataset is of standard size for neuroimaging data and thus serves as a benchmark for ordinary operation. The other two datasets are intended to test the computational limits of the models by using either a large number of trials (MEG super-subject) or a large number of features (fMRI). For the single-subjects dataset, classification performance was measured in addition to training time.
To be as unbiased as possible, hyperparameters were mostly unchanged except when a change made the models more comparable across toolboxes (e.g. setting the same regularization value). No hyperparameter tuning was performed in order to quantify pure training time.

The MVPA-Light models were compared to LIBSVM,  LIBLINEAR, and MATLAB 2019a's fitcdiscr (LDA), lassoglm (LogReg), fitcnb (Naive Bayes), fitcsvm (SVM), ridge, and fitrsvm (SVR). Python and R-based toolboxes were installed in virtual environments using Anaconda 4.7.12. Scikit Learn 0.21.2 was used together with Python 3.7.3. R version 3.6.1 was used with packages MASS (LDA), glmnet (LogReg and Ridge), e1071 (Naive Bayes, SVM, SVR), and listdtr (Kernel Ridge).

All analyses were conducted after a fresh restart of a desktop computer with networking disabled. The computer had an Intel Core i7-6700 @ 3.40 GHz x 8 CPU with 64 GB RAM running on Ubuntu 18.04. All scripts are available in the accompanying GitHub repository\footnote{https://github.com/treder/MVPA-Light-Paper}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% -------- RESULTS --------   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results}

\subsubsection{MEEG data}
\prettyref{fig:results} depicts the results of the MVPA, averaged across subjects. The numbers in the headings correspond to the numbers of the research questions. Errorbars depict standard error across subjects.

\textit{ERP classification}. The bar graph shows that for both the N170 and the sustained ERP component classification accuracy is significantly above the chance level of 33\%. Accuracy is a summary measure that can be broken down into confusion matrices that show which combinations of classes get misclassified ('confused'). For both N170 and the sustained ERP component, the highest accuracy is obtained for the scrambled images. Moreover,  misclassification (off-diagonal elements) is most prominent for the famous and unfamiliar faces. This is not surprising since both types of images are identical in terms of low-level features and both show actual faces, in contrast to the scrambled images. 

\textit{Time classification}. AUC is plotted as a function of time for famous vs scrambled images. The classification was repeated for the three channel sets. The classes are not discriminable prior to the occurrence of the N170. A classification peak at the time of the N170 can be seen for all channel sets. At this stage, the AUC values diverge, with EEG yielding a significantly lower AUC. Combining EEG+MEG seems to yield a slightly higher performance than MEG alone.

\textit{Time-frequency classification}. AUC is plotted as a function of both time and frequency. The AUC values are color-coded. For famous vs scrambled faces, peak performance is reached in the delta frequency band at a latency between 0.2 and 0.4 s. For famous vs unfamiliar faces, peak performance is attained in the latter half of the trial (0.5 - 1 s) in the theta and alpha frequency bands. 

\textit{Generalization}. The first plot depicts AUC (color-coded) as a function of the training time (y-axis) and testing time (x-axis). There is evidence for widespread time generalization starting about at the time of the N170 peak and covering most of the remaining trial. In particular, there is generalization between the N170 and the later sustained component (horizontal and vertical lines emanating at 0.17 s), suggesting some correlation between the spatial pattern of the N170 and the sustained component. 
The second plot depicts AUC as a function of frequency. The is some generalization in the theta band (lower-left corner), the alpha band, and the lower beta band (16--22 Hz). Also, when the classifier is trained in the beta band, classification performance partially generalizes to the alpha band. However, the overall performance is low when compared to the time-locked data. 

\textit{Level 2 statistics}. Group statistical analysis is shown based on the time-frequency classification data in the panel above. Images depict AUC values masked by significance (deep blue = not significant). For the famous vs scrambled faces classification, a large cluster spanning the whole trial and especially the low frequency bands is evident. For the famous vs unfamiliar faces condition, there is a significant  cluster corresponding to large AUC values after 0.5 s and in the lower frequency range.

\textit{Level 1 statistics}. Images depict the AUC values masked by significance. Both uncorrected tests (binomial and permutation test) exhibit spurious effects even at pre-stimulus time. Most of these spurious effects disappear under the cluster permutation test.

\subsubsection{fMRI data}

\prettyref{fig:fmri_results} depicts the results of the MVPA on the fMRI data, averaged across subjects.

\textit{Confusion matrix}. A mask provided with the data was applied to select voxels from ventral temporal areas. 
A high overall performance is observed for LDA with 8 classes. Misclassifications tend to be confined to within more general semantic categories. For instance, misclassified faces tend to be labeled as cats (both living objects), whereas misclassified non-living objects tend to be labeled as other non-living objects. This indicates that there are shared representations for images from the same general category.

\textit{Time classification}. Multi-class classification accuracy was calculated for each time point following stimulus onset. Solid lines show the means across subjects and the shaded areas correspond to standard error. Masks were used to select voxels in the ventral temporal area, voxels responsive to faces, or voxels responsive to houses. Although all ROIs and time points yield performances above the chance level of 12.5\%, the ventral temporal area (which comprises both face and house responsive voxels) yields the best performance. For the latter, classification performance peaks at about 5 s after stimulus onset.

\textit{Searchlight analysis}. AUC values averaged across subjects are depicted. The AUCs are masked by the significant cluster ($p < .01$) and overlayed on an averaged anatomical MRI. Although the cluster is large, high values $>0.8$ are predominantly found in dorsal and ventral visual areas including the paraphippocampal place area and the fusiform area, nicely dovetailing with the original findings of \cite{Haxby2001}.

\subsubsection{Benchmarking}

\prettyref{fig:benchmarking} depicts classification accuracy on the \cite{Wakeman2015ADataset} data calculated for different classifiers and different toolboxes. Except for the MATLAB classifiers, results are nearly identical  for all implementations of LDA, LogReg, and linear SVM, with a peak performance of about 75\%. Lower performance is evident for Naive Bayes, but consistently so across different implementations. For SVM with a RBF kernel, the best performance is obtained in R, followed MATLAB, with both MVPA-Light and Scikit Learn performing worse. Since no hyperparameter tuning was performed, the latter result is most likely due to differences in the default hyperparameters. \prettyref{tab:classification} and \prettyref{tab:regression} show the timing results for different classifiers and regression models. These results are discussed model by model:

\textit{LDA}. The MVPA-Light implementation consistently outperforms other implementations in terms of speed (i.e., lowest training time), in some cases by orders of magnitude. For the fMRI dataset, it is almost 100 times faster than Scikit Learn, whereas MATLAB and R both run out of memory. It is worth noting that a shrinkage value of 0.01 was applied for the MVPA-Light and MATLAB implementations. For R, low performance was achieved with rda (regularized LDA), so the standard unregularized LDA was used. For Scikit Learn, the default solver does not allow for shrinkage so no shrinkage was applied.

\textit{LogReg}. The MVPA-Light implementation of Logistic Regression outperforms the competitors for the MEG single-subjects data. It is outperformed by the R implementation for the MEG super-subject. For the fMRI data, it causes an out of memory error and the best performing model is LIBLINEAR.

\textit{Naive Bayes}. The MVPA-Light implementation consistently outperforms other implementations, in some cases by orders of magnitude. Scikit Learn is consistently second best, followed by R and MATLAB.

\textit{SVM}. For linear SVM, LIBLINEAR yields the best training speed except for the fMRI data, where MVPA-Light performs best. For RBF kernels, MVPA-Light's SVM consistently outperforms the competitors, closely followed by MATLAB's fitcsvm. Significant differences are obtained for different toolboxes, with R being the slowest in many cases. The good performance of MVPA-Light's SVM may appear surprising at first glance, given some of its contenders run using C code. First, MVPA-Light uses a coarse tolerance value; this implies that its algorithm might perform fewer iterations than LIBSVM, although this has not been investigated. As \prettyref{fig:benchmarking} shows, if this is the case, it does not seem to be detrimental to classification performance. Second, the advantages of LIBSVM might not play out during a single training iteration. It has an integrated cross-validation procedure, which is likely to be substantially faster than cross-validation using MVPA-Light, although this has not been investigated either.

\textit{Ridge and Kernel Ridge}. MVPA-Light's models lead the field except for the fMRI data, where Scikit Learn's kernel ridge outperforms MVPA-Light. No results are available for R's krr model; it does not appear to have an interface for fixing hyperparameters and instead performs an expensive search using leave-one-out cross-validation, so it was omitted.

\textit{SVR}. MVPA-Light exclusively relies on LIBSVM for SVR, which leads the field except for one case, in which it closely trails the MATLAB implementation. Overall, R yields the slowest implementation. 

\begin{table}
\footnotesize
\begin{tabular}{ |p{1.1cm}|p{1.4cm}|p{2cm}|p{2.1cm}|p{2cm}|p{2.1cm}|p{2cm}|}
 \hline
 Dataset & Toolbox & \multicolumn{5}{c}{Classifier}\\ \hline
\multicolumn{2}{|c|}{}  & LDA & LogReg & Naive Bayes & SVM (linear) & SVM (RBF)\\
 \hline
\multirow{6}{1.3cm}{MEG single-subjects} & MVPA-Light & $\mathbf{0.003}\pm 0.0001$ & $\mathbf{0.0097}\pm0.0005$ & $\mathbf{0.001}\pm0.00004$ & $0.07\pm0.002$ & $\mathbf{0.02}\pm0.0001$\\
\cline{2-7}
  & LIBLINEAR & - & $0.014\pm0.0009(p)$ $0.035\pm0.001 (d)$ & - & $\mathbf{0.023}\pm0.002(p)$ $0.231\pm0.02(d)$ & -\\
\cline{2-7}
  & LIBSVM & - & - & - & $0.098\pm0.01$ & $0.125\pm0.001$\\
\cline{2-7}
  & MATLAB & $0.026\pm0.0008$ & $0.03\pm0.006$ & $0.05\pm0.0001$ & $0.041\pm0.004$ & $0.023\pm0.0004$\\
\cline{2-7}
   & Scikit Learn & $0.097\pm0.0006$ & $0.1\pm0.005$ & $0.007\pm0.0001$ & $0.37\pm0.052$ & $0.45\pm0.032$\\
\cline{2-7}
  & R & $0.084\pm 0.0003$ & $0.013\pm0.002$ & $0.04\pm0.0001$ & $0.71\pm0.113$ & $0.41\pm0.026$ \\\hline
 \hline
 \multirow{6}{1.3cm}{MEG super-subject} & MVPA-Light & $\mathbf{0.026}\pm0.0028$ & $0.437\pm0.0062$ & $\mathbf{0.015}\pm0.0001$ & $10.122\pm1.05$ & $\mathbf{5.369}\pm0.033$\\
\cline{2-7}
  & LIBLINEAR & - & $0.732\pm0.068(p)$ $0.998\pm0.063(d)$& - & $\mathbf{1.338}\pm0.168(p)$ $6.29\pm0.519(d)$& -\\
\cline{2-7}
  & LIBSVM & - & - & - & $42.089\pm4.188$ & $37.941\pm0.404$\\
\cline{2-7}
  & MATLAB & $0.149\pm0.002$ & $0.279\pm0.137$ & $0.231\pm0.027$ & $20.98\pm1.78$ & $11.65\pm0.217$\\
\cline{2-7}
   & Scikit Learn & $0.596\pm0.017$ & $2.065\pm0.109$ & $0.09\pm0.001$ & $32.19\pm2.07$& $34.56\pm0.38$\\
\cline{2-7}
  & R & $0.84\pm.004$ & $\mathbf{0.159}\pm0.018$ & $0.144\pm.0006$ & $1123.16\pm27.39$ & $123.31\pm9.38$\\\hline
  \hline
\multirow{6}{1.3cm}{fMRI} & MVPA-Light & $\mathbf{0.293}\pm0.0078$ & OOM & $\mathbf{0.309}\pm0.011$ & $\mathbf{0.182}\pm0.0086$ & $\mathbf{2.064}\pm0.235$\\
\cline{2-7}
  & LIBLINEAR & - & $\mathbf{4.008}\pm0.627(p)$ $6.689\pm1.018(d)$ & - & $2.235\pm0.218(p)$ $6.125\pm0.995(d)$ & - \\
\cline{2-7}
  & LIBSVM & - & - & - & $11.79\pm0.787$ & $11.88\pm0.822$ \\
\cline{2-7}
  & MATLAB & OOM & $23.79\pm4.008$ & $357.49\pm2.205$ & $5.053\pm0.325$ & $4.845\pm0.308$\\
\cline{2-7}
   & Scikit Learn & $24.45\pm1.1$ & $20.68\pm4.24$ & $2.86\pm0.06$ & $10.46\pm0.59$ & $9.15\pm0.59$\\
\cline{2-7}
  & R & OOM & $7.1\pm1.13$ & $18.48\pm0.35$ & $39.67\pm1.98$ & $43.3\pm2.18$ \\\hline
\end{tabular}
\caption{Benchmarking results: Mean training time and standard deviation in seconds for different classifiers. For each combination of dataset and classifier, the fastest model is marked in bold. Abbreviations: OOM = out of memory error, (p) = primal form, (d) = dual form.}
\label{tab:classification}
\end{table}

% --- REGRESSION ---
\begin{table}
\footnotesize
\begin{tabular}{ |p{1.1cm}|p{1.8cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
 \hline
 Dataset & Toolbox  & \multicolumn{4}{c|}{Regression model}\\ \hline
\multicolumn{2}{|c|}{}  & Ridge & Kernel Ridge & SVR (linear) & SVR (RBF)\\
 \hline
\multirow{4}{1.3cm}{MEG single-subjects} & MVPA-Light  & $\mathbf{0.0016}\pm0.00006$ & $\mathbf{0.019}\pm0.0001$ & - & -\\
\cline{2-6}
  & LIBSVM & - &  - & $0.02\pm0.001$ & $\mathbf{0.0041}\pm0.0002$ \\
\cline{2-6}
  & MATLAB & $0.0061\pm0.0002$ & - & $\mathbf{0.018}\pm0.037$ & $0.023\pm0.0005$\\   %$\pm$
\cline{2-6}
  & Scikit Learn & $0.0069\pm0.0003$ & $0.023\pm0.003$ & $0.654\pm0.0647$ & $0.481\pm0.02$\\
\cline{2-6}
  & R & $0.055\pm0.0027$ & - & $1.59\pm0.094$ & $0.43\pm0.002$ \\\hline
 \hline
 \multirow{4}{1.3cm}{MEG super-subject} & MVPA-Light & $\mathbf{0.015}\pm0.001$ & $\mathbf{7.38}\pm0.023$ & - & -\\
\cline{2-6}
  & LIBSVM & - &  - & $\mathbf{0.653}\pm0.038$ & $\mathbf{0.121}\pm0.014$ \\
\cline{2-6}
  & MATLAB & $0.186\pm0.007$ & - & $6.931\pm0.237$ & $9.9798\pm0.239$\\
\cline{2-6}
  & Scikit Learn & $0.062\pm0.005$ & $14.51\pm0.21$ & $3.213\pm0.394$ & $31.61\pm1.51$\\
\cline{2-6}
  & R & $0.547\pm0.0079$ & - & $465.08\pm49.83$ & $151.66\pm26.76$\\\hline
  \hline
\multirow{4}{1.3cm}{fMRI} & MVPA-Light & $\mathbf{0.165}\pm0.0042$ & $2.026\pm0.256$ & - & -\\
\cline{2-6}
  & LIBSVM & - &  - & $\mathbf{4.334}\pm1.48$ & $\mathbf{2.819}\pm0.0412$ \\
\cline{2-6}
  & MATLAB & OOM & - & $4.545\pm0.353$  & $4.563\pm0.284$\\
\cline{2-6}
  & Scikit Learn & $0.638\pm0.022$ & $\mathbf{0.476}\pm0.01$ & $16.138\pm3.64$ & $9.999\pm0.59$\\
\cline{2-6}
  & R & $7.503\pm0.593$ & - & $37.211\pm2.056$ & $41.037\pm2.298$\\\hline
\end{tabular}
\caption{Benchmarking results: Training time for different regression models.}
\label{tab:regression}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% -------- DISCUSSION --------   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

MVPA-Light offers a suite of classifiers, regression models and metrics for multivariate pattern analysis. In addition, a high-level interface allows to carry out common MVPA tasks such as cross-validated classification across time,  generalization, and searchlight analysis. The toolbox supports hyperparameter tuning and pre-computed kernels. The statistical significance of the MVPA result can be established using level 1 and level 2 tests.
%Since the toolbox is entirely statistical (i.e. it just operates on data matrices), it can easily be combined with MATLAB-based neuroimaging toolboxes. 

% Preprocessing
MVPA-Light also provides a nested preprocessing pipeline that applies operations to training and test sets separately. Among others, it features over- and undersampling, PCA, and scaling operations. It also includes an averaging approach wherein samples are assigned to groups and then averaged in order to increase signal-to-noise ratio. For linear classifiers, this approach has been explored by \citep{Cichy2015,Cichy2017MultivariateSpace}. Recently, it has been generalized to non-linear kernel methods \citep{Treder2018}. Either approach can be used in the toolbox by adding the operation \ttt{average\_samples} or \ttt{average\_kernel} to the preprocessing pipeline. 
% Wakeman & Henson and Haxby analysis
To showcase some of its features, analyses of an MEEG \citep{Wakeman2015ADataset} and an fMRI \citep{Haxby2001} dataset are reported. The results illustrate some ways in which the toolbox can aid in quantifying the similarity of representations, measuring the information content, localizing discriminative information in the time-frequency plane, highlighting shared representations across different time points or frequencies, and establishing statistical significance.

% Benchmarking
A benchmarking analysis was conducted on three different datasets in order to compare MVPA-Light (including LIBSVM and LIBLINEAR) to models provided in the MATLAB Statistics Toolbox, various R packages, and Scikit Learn  for Python. While classification performance is largely consistent across different platforms, training time varies considerably. The MVPA-Light implementations of LDA, Naive Bayes, and Ridge Regression consistently outperform  their competitors, in some cases by orders of magnitude. For Logistic Regression and SVM, the MVPA-Light implementations and LIBLINEAR lead the field. In all but one case, MVPA-Light's classifiers are faster than the contenders in MATLAB, R, and Scikit Learn. Overall, the fastest classifier is MVPA-Light's LDA and the fastest regression model is MVPA-Light's Ridge Regression. Partially, the success of MVPA-Light is due to specialization: MVPA-Light models tend to have fewer hyperparameters than other models, and MVPA-Light features separate optimized implementations for binary LDA and multi-class LDA, whereas the other toolboxes have a single implementation. Furthermore, MVPA-Light's LDA and Ridge Regression dynamically switch between primal and dual form. This can increase computational efficiency especially when dealing with a large datasets.

The benchmarking results should not be interpreted as final verdicts on the respective toolboxes. 
 Undoubtedly, training speed can be improved by finding an optimal set of hyperparameters for a model. For instance, increasing regularization tends to lead to smoother loss surfaces and often faster convergence for gradient descent algorithms. The strategy for the present analysis was to change default parameters minimally and, if so, only in order to increase comparability, e.g. by setting a regularization parameter to a common value. 
 Although MVPA-Light will likely perform well in other situations, too, the present results are mostly indicative of \textit{default performance}, obtained with minimal user interference. This is a relevant measure since it is our belief that the burden of hyperparameter selection should be taken off the user as much as possible.


%Implementations of the same model in different software packages can use different solvers, different regularization techniques, and have a different set of default hyperparameters and capabilities that may cause additional overhead during optimization.
%These results should not be interpreted as final verdicts on the corresponding models. 

% % Statistics
% Second-level statistical analyses are not currently covered by the toolbox. Such statistics can be obtained by combining e.g. the cluster permutation statistics framework in FieldTrip \citep{Maris2007} with MVPA-Light. For single subjects, a binomial test for accuracy is implemented in the function \ttt{mv\_statistics}. For the future, it is planned to include a second-level statistical framework based on cluster permutation tests directly in the toolbox. Since permutation tests are computationally expensive, the framework is planned to include recent developments such as a fast permutation scheme for LDA and kernel FDA \citep{Treder2019DirectFDA}.

\subsection{\red{Setting up a MVPA pipeline}}

\red{If one is spoilt for choice, selecting  a model, metrics, and preprocessing steps can be challenging. This section offers practical advice in this regard. Such recommendations tend to be subjective to some extent, hence users are encouraged to perform their own MVPA experiments and compare different models, hyperparameter settings etc. To prevent a statistical bias, extensive experiments should not be performed on the dataset at hand. Instead, a similar dataset e.g. recorded using the same hardware with a similar paradigm can be used for experimentation.\\} 

\red{\textit{Preprocessing the data:} Although MVPA can be applied to raw data,  this may negatively affect performance, so data has ideally been cleaned and corrupted trials have been rejected. It is useful to normalize the data for numerical stability by e.g. z-scoring each feature to mean = 0 and standard deviation = 1. This is particularly important for Logistic Regression which uses the exponential function. It also applies to LDA and kernel methods because lack of normalization can lead to results being dominated by the features with the largest scaling.  Normalization can be applied on the whole dataset, unless the goal is to simulate online performance (e.g. in a brain-computer interface). In this case, it should be performed nested inside the  cross-validation by using the \ttt{cfg.preprocess} field. All preprocessing steps involving the class labels, such as CSP \citep{Blankertz2008a}, also need to be performed in a nested fashion. Furthermore, for kernel methods, computation can be speeded up by precomputing the kernel matrix using \ttt{compute\_kernel\_matrix}, although this approach does not work when time generalization is required.}

\red{\textit{Choosing a classifier:} Linear classifiers perform well in a large variety of tasks. LDA is a good default model, since it is computationally efficient and robust thanks to regularization \citep{Blankertz2011}. Logistic Regression and linear SVM are more resilient to outliers than LDA, so may be preferred for noisy or strongly non-Gaussian data. Logistic Regression has a hyperparameter-free regularization by default, hence it is more user-friendly than SVM which requires setting the hyperparameter $c$. Naive Bayes should only be used after the features have been  decorrelated using PCA or ICA. For non-linear problems, kernel FDA or SVM can be used. Again, SVM requires $c$ to be set, whereas for kernel FDA the default regularization often works well. Regarding the choice of a kernel, the RBF kernel is adequate for most classification tasks, but its hyperparameter \ttt{gamma} determining the kernel width might require tuning. If maximizing classification accuracy is vital, it is worth to try an ensemble of classifiers. }

\red{\textit{Choosing a regression model:} Ridge regression tends to perform well on a variety of tasks. If the data is noisy, linear Support Vector Regression (SVR) using LIBLINEAR can be applied. If the problem is clearly non-linear, either kernel ridge or kernel SVR using LIBSVM using a RBF kernel is recommended. }

\red{\textit{Metrics:} The most common classification metric is accuracy. For multi-class problems, it is useful to complement it with a confusion matrix. For two classes, AUC is a good alternative to accuracy since it is more robust to class imbalances and invariant to shifts of the classifier threshold. When the roles of the  classes are asymmetric (e.g. patients vs controls), it is useful to report precision and recall along with their harmonic mean (F1 score). If in doubt, report multiple metrics.}

\red{\textit{Cross-validation:} Classification and regression results should be cross-validated in order to control for overfitting. Cross-validation is performed by default in all high-level functions. Unless the number of samples is very small, leave-one-out cross-validation should be avoided because it suffers from a large bias; instead, 5-fold or 10-fold cross-validation should be used by setting \ttt{cfg.k = 5} or \ttt{cfg.k = 10} \citep{James2013}. Since the folds are randomly assigned, repeating the cross-validation is recommended to get a more stable estimate. This can be done by e.g. setting \ttt{cfg.repeat = 2}.}

\subsection{Conclusion}

MVPA-Light is a comprehensive toolbox for multivariate pattern analysis. Its models perform competitively compared to other implementations. Future development of MVPA-Light will include additional feature extraction techniques for oscillations, such as Common Spatial Patterns \citep{Blankertz2008a} and the Riemannian geometry approach \citep{Barachant2013}, and further computational improvements, such as efficient permutation testing for LDA/KFDA \citep{Treder2019DirectFDA} and faster calculation of the regularization path for SVM \citep{Hastie2004TheMachine}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% --------- APPENDIX ---------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix}


%%% -------- LDA --------
\subsection{LDA implementation}\label{sec:lda_implementation}

\red{The implementation of LDA in MVPA-Light differs from the generative model presented in \prettyref{sec:lda}. It uses the more efficient Fisher Discriminant Analysis (FDA)  \citep{Fisher1936} which omits calculating probabilities and operates in two steps.} In the first step, the data is mapped onto a $(c-1)$-dimensional subspace, where $c$ is the number of classes. In the second step, a sample is assigned to the class with the closest class centroid. LDA thus acts as a prototype classifier within this subspace. The coordinates for the mapping are found by iteratively solving the equation

\begin{equation}
\label{eq:fda}
\w_{\text{lda}} = \underset{\w}{\text{arg max}}\ \frac{\w^\top \S_b\w}{\w^\top \S_w\w}
\end{equation}

where $\S_b$ and $\S_w$ are defined as

\begin{equation*}
%\label{eq:scatter-means-multi}
\begin{alignedat}{2}
\S_b =\ & \sum_{j\,\in\{1,2,...,c\}}n_j\,(\mm{j} -\mbar) (\mm{j} - \mbar)^\top\ \quad &&\text{(between-classes scatter)}\\
\S_w =\ & \sum_{j\,\in\{1,2,...,c\}}\sum_{i\in\mathcal{C}_j} (\x_i - \mm{j})(\x_i - \mm{j})^\top\  \quad &&\text{(within-class scatter)}\\
\end{alignedat}
\end{equation*}

Here, $n_j$ is the number of instances in class $j$, $\m_j$ is the $j$-th class mean, $\mbar$ is the sample mean, and $\mathcal{C}_j$ is the set of indices of instances in class $j$. \red{Note that $\S_w$ is simply the un-normalized version of $\mathbf{\Sigma}$ in  \prettyref{eq:lda_likelihood}}. For two classes, LDA has the simple solution 

\begin{equation}
\label{eq:lda_solution}
\w_{\text{lda}} = \S_w^{-1}\ (\m_1 - \m_2).
\end{equation}

For more than two classes, there is multiple vectors $\w_{\text{lda}}$. They are collected in a matrix $\W\in\R^{p\times(c-1)}$ and scaled such that $\W^\top\S_w\W = \I$ \citep{Bishop2007}. $\W$ can be obtained via the generalized eigenvalue problem $\S_b\,\W = \S_w\,\W\mathbf{\Lambda}$ where $\mathbf{\Lambda}$ is a diagonal matrix of eigenvalues. 
The threshold is calculated under the assumption of equal probabilities for the classes. $\S_w$ is often ill-conditioned or singular and hence the inverse in \prettyref{eq:lda_solution} cannot be calculated reliably. Therefore, shrinkage regularization is applied by default and $\S_w$ is replaced by $\widetilde{\S}_w$:

\begin{align}
\label{eq:shrinkage}
\widetilde{\S}_w = (1-\lambda)\ \S_w + \lambda\,\nu\,\I
\end{align}

where $\I$ is the identity matrix and $\nu = \text{trace}(\S_w)/p$ \citep{Blankertz2011}. For $\lambda=1$, LDA becomes a prototype classifier, that is, each sample is assigned to the closest centroid in feature space. \red{The effect of shrinkage regularization is depicted in \prettyref{fig:lda}}. In the literature, LDA has also been used with ridge-regression type regularization, yielding $\widetilde{\S}_w = \S_w + \lambda_\text{ridge}\,\I$ with $\lambda_\text{ridge}\in [0,\infty)$ \citep{Friedman1989RegularizedAnalysis}. The user can switch to ridge regularization by setting \ttt{reg = 'ridge'}. However, both regularization approaches are equivalent up to scaling. For a given shrinkage value $\lambda<1$, the corresponding ridge regularization value is $\lambda_\text{ridge} = \nu\lambda / (1-\lambda)$.

If $p \gg n$, that is, the number of features is much larger than the number of samples, calculation and inversion of the [features $\times$ features] covariance matrix can be computationally expensive. In these cases, the [samples $\times$ samples] Gram matrix can be used instead. The hyperparameter \ttt{form} decides on whether the 'primal' form based on covariance or 'dual' form based on the Gram matrix is used. By default (\ttt{form = 'auto'}), this is decided automatically by comparing $n$ and  $p$. The resultant formula is equivalent to kernel FDA with a linear kernel. However, the regularization of $\mathbf{N}$ in  kernel FDA is \textit{not} equivalent to the regularization of $\S_w$. Furthermore, to the best of our knowledge, the Ledoit-Wolf estimate has not been reported in the literature using dual notation. Therefore, a dual regularization approach for LDA is developed next.

\subsubsection{Dual regularization of LDA}\label{app:dualLDA}

The goal of this section is to develop a dual regularization approach for LDA that is equivalent to the primal regularization approach. This is important in order to assure that switching from primal to dual form with an equal regularization magnitude $\lambda$ does not affect the solution $\w$. The approach consists of two steps. First, the Ledoit-Wolf estimate for the optimal regularization hyperparameter $\lambda^*$ needs to be calculated using the Gram matrix instead of the covariance matrix. Second, it needs to be assured that the dual regularization yields the same $\w$ as the primal approach.

\subsubsection{Dual Ledoit-Wolf estimate}

The Ledoit-Wolf estimate \citep{Ledoit2003HoneyMatrix,Blankertz2011} for the optimal regularization hyperparameter, denoted as $\lambda^*$, can be formulated as 

\begin{equation}\label{eq:LedoitWolf}
\lambda^* = \frac{\sum_{i=1}^n ||\S - \x_i\x_i^\top||_F^2}%
{n^2 [\,\Tr{\S^2} - \Tr{\S}^2/p\,]}
\end{equation}

where $\mathbf{S}= \frac{1}{n}\X^\top\X$, $\text{Tr}$ is the trace operator and $\X$ is assumed to be zero mean (column-wise). In practice, $\lambda^*$ can take values smaller than 0 or larger than 1 in finite sample, so it is additionally being thresholded between 0 and 1. 

To rewrite \prettyref{eq:LedoitWolf} in terms of the Gram matrix $\K= \X\X^\top$ observe that in the numerator we can write $||\S - \x_i\x_i^\top||_F^2 = \Tr{\K^2}/n^2 - 2\,\Tr{\S\,\x_i\x_i^\top} + \langle\x_i,\x_i\rangle^2$. For the middle term we have $\Tr{\S\,\x_i\x_i^\top} = \Tr{\frac{1}{n}\sum_{j=1}^n \x_j\x_i^\top\langle\x_i,\x_j\rangle} = \frac{1}{n}\sum_{j=1}^n \langle\x_i,\x_j\rangle^2$. In other words, we can rewrite the Ledoit-Wolf estimate in terms of inner products as 

\begin{equation}\label{eq:LedoitWolfDual}
\lambda^* = \frac{\Tr{\K^2}/n - 2\sum_{i=1}^n\sum_{j=1}^n \K_{ij}^2/n + \sum_{i=1}^n \K_{ii}^2 }%
{\Tr{\K^2} - \Tr{\K}^2/p}
\end{equation}

where the identity $\Tr{\S^2} = \Tr{\K^2}/n^2$ has been used. Both primal and dual estimation of $\lambda^*$ are implemented in the function \ttt{LedoitWolfEstimate}. This approach generalizes to kernel FDA when the Gram matrix is replaced by the kernel matrix.


\subsubsection{Dual regularization}

The relationship between primal weights $\w$ and dual weights $\al$ is given by $\w = \X^\top\al$. The denominator of the Fisher ratio in \prettyref{eq:fda} can be related to the dual problem as $\w^\top\S_w\w = \al^\top\mathbf{N}\al$ where $\mathbf{N}$ is the "dual" of $\S_w$ (see \prettyref{eq:kernelFDA}). For the shrinkage regularized case, we then obtain 

\begin{align}
\begin{split}
\label{eq:dualLDA}
\w^\top(\S_w + \lambda\I)\,\w =&\ \al^\top(\mathbf{N} + \lambda\K)\al
\end{split}
\end{align}

where $\K$ is the Gram matrix as defined above. The optimal $\al$ is given by $(\mathbf{N} + \lambda\K)^{-1}(\M_1 - \M_2)$ where $\M_j$ is defined as in \prettyref{eq:kernelFDA}.

%%% -------- NAIVE BAYES --------
\subsection{Naive Bayes implementation}\label{sec:naive_bayes_implementation}

MVPA-Light uses Gaussians to model the univariate densities, that is $P(\xf{j}\,|\,y = i) = \frac{1}{\sqrt{2\pi\sig{i}{j}}} \exp(-\frac{(\xf{j} - \mf{i}{j})^2}{2\sig{i}{j}})$. The parameters of the model are thus the $\mf{i}{j}$'s (mean of the $j$-th feature in class $i$) and $\sig{i}{j}$'s (variance of the $j$-the feature in class $i$) estimated on the training data. Inserting the Gaussian densities into \prettyref{eq:bayes} yields

\begin{equation}
\label{eq:naive_bayes_gaussian}
P(y = i\,|\,\x) = \frac{\prod_{j=1}^p \exp(-\frac{(\xf{j} - \mf{i}{j})^2}{2\sig{i}{j}})\,P(y = i)}%
{\sum_{k=1}^c \prod_{j=1}^p \exp(-\frac{(\xf{j} - \mf{k}{j})^2}{2\sig{k}{j}})\,P(y = k)}.
\end{equation}

Classification is computationally more efficient if the denominator (which is necessary for normalization only) is omitted and the numerator is log transformed. This leads to the class-conditional decision values $d_i$,

\begin{equation}
\label{eq:naive_bayes_dval}
d_i = \log P(y = i) - \sum_{j=1}^p \frac{(\xf{j} - \mf{i}{j})^2}{2\sig{i}{j}}
\end{equation}

which can be transformed into posterior probabilities using the softmax function 

\begin{equation}
\label{eq:naive_bayes_softmax}
P(y = i\,|\,\x) = \frac{e^{d_i}}{\sum_{k=1}^c e^{d_k}}.
\end{equation}

%%% -------- LOGISTIC REGRESSION --------
\subsection{Logistic Regression implementation}\label{sec:logreg_implementation}

\red{The optimization problem underlying Logistic Regression is convex but there exists no analytical solution}. Instead, an iterative algorithm is required to optimize $\w$. In MVPA-Light, the Trust Region Newton Method introduced by \cite{Lin2007TrustRegression} is implemented in the function \ttt{TrustRegionDoglegGN}. \red{Log-F(1,1) regularization (\ttt{reg = 'logf'}) is implemented via data augmentation. L2-regularization is implemented by adding a penalty term to the loss function:}

\begin{equation}
\label{eq:logreg_loss_function_plus_penalty}
\L_\text{LR}^{L2}(\w) = \L_\text{LR}(\w) + \frac{\lambda}{2}\, ||\w||^2
\end{equation}

Here, $\lambda\in [0,\infty)$ controls the amount of regularization. While log-F(1,1) regularization does not require any hyperparameters, L2-regularization requires $\lambda$ to be set. \red{The effect of regularization on the predicted probabilities is depicted in \prettyref{fig:logreg}. If multiple candidates are provided for $\lambda$, a line search is performed using nested cross-validation. This kind of line search is costly, since the classifier has to be trained multiple times for each value of the hyperparameter. To speed up the search, 'warm starts' can be used wherein the initial value for $\w$, denoted as  $\w_\text{init}$, is a function of the solutions in previous iterations.} If \ttt{predict\_regularization\_path=1}, then in the $k$-th iteration, a polynomial function is fit to the solutions $\w_{k-1},\w_{k-2},...$ to the previous iterations  using $\lambda_{k-1}, \lambda_{k-2}, ...$ as predictors. It is then evaluated at the current value of $\lambda$ in order to predict a good starting vector for the optimization.


%%% -------- SVM --------
\subsection{SVM implementation}\label{sec:svm_implementation}

\red{The formulation of SVM in \prettyref{eq:svm_primal} is intuitive but limited to the linear case. It can be rewritten into a dual formulation which applies to both linear and kernel case  \citep{Hsieh2008ASVM}. The optimal weights are then found by solving the quadratic optimization problem }

\begin{align}
\begin{split}
\label{eq:svm_dual_loss_function}
\underset{\al}{\text{arg min}}\quad
& \frac{1}{2} \al^\top\Q\,\al - \mathbbm{1}^\top\al\\
\text{subject to}\quad  &\ \forall i: 0 \le \alpha_i \le c
\end{split}
\end{align}

where $\al=[\alpha_1,\alpha_2,...,\alpha_n]^\top\in\R^n$ is the dual weight vector and  $\mathbbm{1}$ is a vector of 1's. $\Q$ is the kernel matrix with the class labels absorbed, i.e. $\Q_{ij} = y_i y_j\, k(\x_i,\x_j)$, where $k$ is the kernel function, and $y_i, y_j \in\{+1, -1\}$ are the class labels for the $i$-th and $j$-th sample. The hyperparameter \ttt{c} controls the amount of regularization. The optimization of the weights is performed using a Dual Coordinate Descent approach  \citep{Hsieh2008ASVM} implemented in the function \ttt{DualCoordinateDescent}. 


%%% -------- KERNEL FDA --------
\subsection{KFDA implementation}

Let $\K\in\R^{n\times n}$ be the kernel matrix representing the inner products of the samples in the Reproducing Kernel Hilbert Space (RKHS) and $\K_j\in\R^{n \times n_j}$ be the submatrix of $\K$ with columns corresponding to samples in class $j$. The kernelized versions of between-classes and within-class scatter are given by the matrices $\M$ and $\mathbf{N}$ as


\begin{equation}
\label{eq:kernelFDA}
\begin{alignedat}{2}
\M =\ &  \sum_{j=1}^c n_j\ (\M_j - \M_*)\,(\M_j-\M_*)^\top\\
\mathbf{N} =\ & \sum_{j=1}^c \K_j\ (\mathbf{I} - \mathbf{1})\  \K_j^\top
\end{alignedat}
\end{equation}

where $\M_j\in\R^n$ is the mean of the columns of $\K_j$ and $\M_*$ is the mean of the columns of $\K$. Then the solution is given by the $(c-1)$ leading eigenvectors of $\mathbf{N}^{-1} \mathbf{M}$. 
Like in LDA, the model can be regularized using ridge regularization (\ttt{reg = 'ridge'}) or shrinkage regularization (\ttt{reg = 'shrink'}). Using shrinkage, $\mathbf{N}$ is replaced by $\widetilde{\mathbf{N}} = (1-\lambda)\ \mathbf{N} + \lambda\,\nu\,\I$.


%%% -------- PROTOTYPE CLASSIFIER --------
\subsection{Implementing a prototype classifier}

This section illustrates how to implement a prototype classifier that assigns a new sample to the class with the closest class centroid. Class centroids are calculated from the training data by calculating the means of the samples within each class. This example is solely didactic. A prototype classifier can be simulated with LDA or multi-class LDA when setting \ttt{reg = 'shrink'} and \ttt{lambda = 1}.

First, the train function needs to be implemented. As input arguments, train functions take a \ttt{param} struct which is short notation for 'hyperparameter'. It corresponds to the \ttt{cfg.hyperparameter} struct defined in the high-level functions. Additionally, it takes the training data \ttt{X} and the corresponding class labels \ttt{clabel}. For brevity, most of the documentation is omitted.

\begin{verbatim}
    function model = train_prototype(param,X,clabel)

    nclasses = max(clabel);

    % Classifier struct
    model  = [];

    %% Calculate class centroids
    model.centroid = zeros(nclasses, size(X,2));
    for c=1:nclasses
        model.centroid(c,:) = mean(X(clabel==c,:));
    end
\end{verbatim}

The output of the train function is a structure \ttt{model} that describes the parameters of the classifier after training. The test function takes the classifier \ttt{model} and the test data \ttt{X} as input. For classification, we need to calculate the Euclidean distance between each test sample and each of the centroids. Then, each sample is assigned to the closest class centroid.

\begin{verbatim}
    function clabel = test_prototype(model,X)

    % Euclidean distance of each sample 
    % to each class centroid
    dist = arrayfun( @(c) sum( bsxfun(@minus, dval, ...
        model.centroid(c,:)).^2, 2), ...
        1:model.nclasses, 'Un',0);
    dist = cat(2, dist{:});

    % For each sample, find the closest centroid
    clabel = zeros(size(X,1),1);
    for ii=1:size(X,1)
        [~, clabel(ii)] = min(dist(ii,:));
    end

\end{verbatim}

The output of the test function is \ttt{clabel}, the vector of predicted class labels. Finally, an entry for \ttt{prototype} needs to be added to the function \ttt{mv\_get\_hyperparameter}. Since the prototype classifier has no hyperparamaters, this entry can be empty. Provided that the train and test functions are in the MATLAB path, the new classifier can be used with all high-level functions by setting \ttt{cfg.model = 'prototype'}.


\section*{Conflict of Interest Statement}
%All financial, commercial or other relationships that might be perceived by the academic community as representing a potential conflict of interest must be disclosed. If no such relationship exists, authors will be asked to confirm the following statement: 

The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.

\section*{Author Contributions}

MT developed the toolbox, performed all analyses and authored the manuscript.

\section*{Funding}
NA

\section*{Acknowledgments}
I would like to thank colleagues from the Psychology department at University of Birmingham for advice and early adaptation of the toolbox, and Jan-Mathijs Schoffelen and Sophie Arana for their efforts towards integrating it into FieldTrip. Many thanks to all contributors to the GitHub repository
\footnote{\ttt{https://github.com/treder/MVPA-Light/graphs/contributors}}.


\section*{Data Availability Statement}
The MEEG dataset can be found in the OpenNeuro repository (https://openneuro.org/datasets/ds000117/versions/1.0.3). The fMRI dataset can be found on the PyMVPA website (http://www.pymvpa.org/datadb/haxby2001.html). All scripts and figures used in this paper are available in the accompanying GitHub repository (github.com/treder/MVPA-Light-Paper).
% Please see the availability of data guidelines for more information, at https://www.frontiersin.org/about/author-guidelines#AvailabilityofData

\bibliographystyle{frontiersinSCNS_ENG_HUMS} % for Science, Engineering and Humanities and Social Sciences articles, for Humanities and Social Sciences articles please include page numbers in the in-text citations
%\bibliographystyle{frontiersinHLTH&FPHY} % for Health, Physics and Mathematics articles
\bibliography{references}

%%% Make sure to upload the bib file along with the tex file and PDF
%%% Please see the test.bib file for some examples of references

\section*{Figure captions}

%%% Please be aware that for original research articles we only permit a combined number of 15 figures and tables, one figure with multiple subfigures will count as only one figure.
%%% Use this if adding the figures directly in the mansucript, if so, please remember to also upload the files when submitting your article
%%% There is no need for adding the file termination, as long as you indicate where the file is saved. In the examples below the files (logo1.eps and logos.eps) are in the Frontiers LaTeX folder
%%% If using *.tif files convert them to .jpg or .png
%%%  NB logo1.eps is required in the path in order to correctly compile front page header %%%

% \begin{figure}[ht!]
% \centering\includegraphics[width=.6\linewidth]{X_clabel_cfg}
% \caption{The three most commonly used input arguments in MVPA-Light. Colors are used to represent the two classes 1 (turquoise) and 2 (red) for illustrative purposes. Note that \textbf{X} can be three-dimensional (e.g. if it also has a time dimension) or even higher-dimensional (e.g. time-frequency data).}\label{fig:X}
% \end{figure}

\begin{figure}[ht!]
\centering\includegraphics[width=\linewidth]{fig1_MVPA_structure}
\caption{Structure of MVPA-Light.}\label{fig:structure}
\end{figure}

\begin{figure}[ht!]
\centering\includegraphics[width=\linewidth]{overview_classifiers}
\caption{\red{Overview of the available classifiers. Dots represent samples, color indicates the class. \textit{LDA}: different classes are assumed to have the same covariance matrix, indicated by the ellipsoids. \textit{Gaussian Naive Bayes}: features are conditionally independent, yielding diagonal covariance matrices. \textit{Logistic regression}: a sigmoid function (curved plane) is fit to directly model class probabilities. \textit{SVM}: a hyperplane (solid line) is fit such that the margin (distance from hyperplane to closest sample; indicated by dotted lines) is maximized. \textit{Ensemble}: multiple classifiers are trained on subsets of the data. In this example, their hyperplanes partition the data into spaces belonging to classes 1 and 2. After applying all classifiers to a new data point and collecting their 'votes', the class receiving most votes is selected. \textit{Kernel methods}: in this example the optimal decision boundary is circular (dashed circle), hence the data is not linearly separable. After projection into a high-dimensional feature space using a map $\phi$, the data becomes linearly separable (dashed line) and a linear classifier such as SVM or LDA can be successfully applied.}}\label{fig:classifiers}
\end{figure}

\begin{figure}[ht!]
\centering\includegraphics[width=\linewidth]{MVPA_results}
\caption{Results for the classification analysis of the \cite{Wakeman2015ADataset} MEEG data.}\label{fig:results}
\end{figure}

\begin{figure}[ht!]
\centering\includegraphics[width=\linewidth]{fmri_results}
\caption{Results for the classification analysis of the \cite{Haxby2001} fMRI data.}\label{fig:fmri_results}
\end{figure}

\begin{figure}[ht!]
\centering\includegraphics[width=\linewidth]{benchmark_classification}
\caption{Classification results for the benchmarking analysis using the MEG single-subjects data.}\label{fig:benchmarking}
\end{figure}

\begin{figure}[ht!]
\centering\includegraphics[width=\linewidth]{figures/LDA_regularization}
\caption{a) LDA with two classes. Covariance matrices are indicated by ellipses. The weight vector $\w$ is the normal to the hyperplane. b) Effect of varying $\lambda$ on the shape of the covariance matrix. c) Covariance matrices estimated from data usually overestimate large eigenvalues \citep{Blankertz2011}. d) The shrinkage estimate $\widetilde{\S}_w$ partially corrects the estimation error by shrinking towards a spherical covariance $\nu\mathbf{I}$.}\label{fig:lda}
\end{figure}

\begin{figure}[ht!]
\centering\includegraphics[width=\linewidth]{figures/logreg_regularization}
\caption{Logistic regression on data with two features (x and y axis) and two classes represented by purple (class 1) and green (class 2) dots. The curved surface is the sigmoid function fit and the vertical z-axis represents the probability for class 1. In L2 regularization, a larger $\lambda$ leads to a flatter sigmoid function with smoothly varying probabilities. For comparison, the sigmoid fit using Log-F(1,1) regularization is also shown.}\label{fig:logreg}
\end{figure}

%%% If you are submitting a figure with subfigures please combine these into one image file with part labels integrated.
%%% If you don't add the figures in the LaTeX files, please upload them when submitting the article.
%%% Frontiers will add the figures at the end of the provisional pdf automatically
%%% The use of LaTeX coding to draw Diagrams/Figures/Structures should be avoided. They should be external callouts including graphics.
\end{document}